{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6725d40",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End Indicator ML Strategy (BTCUSDT)\n",
    "\n",
    "Blueprint notebook to transform the provided feature-rich CSVs into a clean feature layer, label future returns, train a baseline ML model, and backtest simple probability-to-position mappings. The main focus is the 1H dataset, with comparison against 4H and 1D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86444ff1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Paths\n",
    "Load core dependencies and configure the file locations for each timeframe. The 1H file is treated as the primary source while keeping 4H and 1D available for quick comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def discover_candidate_data_dirs() -> List[Path]:\n",
    "    \"\"\"Enumerate plausible data directories, including env overrides and repo root.\"\"\"\n",
    "    candidates: List[Path] = []\n",
    "    seen = set()\n",
    "\n",
    "    def add(path_like):\n",
    "        path = Path(path_like).resolve()\n",
    "        if path in seen:\n",
    "            return\n",
    "        seen.add(path)\n",
    "        candidates.append(path)\n",
    "\n",
    "    env_dir = os.getenv(\"DATA_DIR\")\n",
    "    if env_dir:\n",
    "        add(env_dir)\n",
    "\n",
    "    cwd = Path.cwd().resolve()\n",
    "    add(cwd / \"data\")\n",
    "    add(cwd / \"notebooks\" / \"data\")\n",
    "    for parent in cwd.parents:\n",
    "        add(parent / \"data\")\n",
    "        add(parent / \"notebooks\" / \"data\")\n",
    "\n",
    "    try:\n",
    "        repo_root = Path(\n",
    "            subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], text=True).strip()\n",
    "        )\n",
    "        add(repo_root / \"data\")\n",
    "        add(repo_root / \"notebooks\" / \"data\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return [p for p in candidates if p.exists()]\n",
    "\n",
    "\n",
    "def resolve_data_path(filename: str) -> Path:\n",
    "    data_dirs = discover_candidate_data_dirs()\n",
    "    attempts = []\n",
    "    for base in data_dirs:\n",
    "        candidate = base / filename\n",
    "        attempts.append(candidate)\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate data file '{filename}'. Checked: {attempts or ['<no data dirs found>']}. \"\n",
    "        \"Set DATA_DIR env var or place the file under one of the listed data directories.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_FILES = {\n",
    "    \"1h\": \"BINANCE_BTCUSDT.P, 60.csv\",\n",
    "    \"4h\": \"BINANCE_BTCUSDT.P, 240.csv\",\n",
    "    \"1d\": \"BINANCE_BTCUSDT.P, 1D.csv\",\n",
    "}\n",
    "DATA_PATHS = {tf: resolve_data_path(filename) for tf, filename in DATA_FILES.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_layer(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found: {path}. Checked candidate data dirs: {discover_candidate_data_dirs()}.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = rename_and_filter_columns(df)\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.sort_values(\"time\").drop_duplicates(subset=[\"time\"])\n",
    "    df = df.set_index(\"time\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.ffill().bfill()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
    "    df[\"bb_percent\"] = (df[\"close\"] - df[\"bb_lower\"]) / (df[\"bb_upper\"] - df[\"bb_lower\"])\n",
    "    df[\"bb_width\"] = (df[\"bb_upper\"] - df[\"bb_lower\"]) / df[\"bb_basis\"]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RENAME_MAP = {\n",
    "    'time': 'time',\n",
    "    'open': 'open',\n",
    "    'high': 'high',\n",
    "    'low': 'low',\n",
    "    'close': 'close',\n",
    "    'Volume': 'volume',\n",
    "    'VWAP': 'vwap',\n",
    "    'Upper Band #1': 'bb1_upper',\n",
    "    'Lower Band #1': 'bb1_lower',\n",
    "    'Basis': 'bb_basis',\n",
    "    'Upper': 'bb_upper',\n",
    "    'Lower': 'bb_lower',\n",
    "    'Up Trend': 'trend_up',\n",
    "    'Down Trend': 'trend_down',\n",
    "    'EMA': 'ema_fast',\n",
    "    'EMA.1': 'ema_slow',\n",
    "    'Conversion Line': 'ichi_conv',\n",
    "    'Base Line': 'ichi_base',\n",
    "    'Lagging Span': 'ichi_lag',\n",
    "    'Leading Span A': 'ichi_span_a',\n",
    "    'Leading Span B': 'ichi_span_b',\n",
    "    'Upper.1': 'channel_upper',\n",
    "    'Average': 'channel_mid',\n",
    "    'Lower.1': 'channel_lower',\n",
    "    'RSI': 'rsi',\n",
    "    'RSI-based MA': 'rsi_ma',\n",
    "    'Regular Bullish': 'div_bull',\n",
    "    'Regular Bullish Label': 'div_bull_label',\n",
    "    'Regular Bearish': 'div_bear',\n",
    "    'Regular Bearish Label': 'div_bear_label',\n",
    "    'Histogram': 'macd_hist',\n",
    "    'MACD': 'macd',\n",
    "    'Signal': 'macd_signal',\n",
    "    'ATR': 'atr',\n",
    "    'K': 'stoch_k',\n",
    "    'D': 'stoch_d',\n",
    "    '%R': 'williams_r',\n",
    "}\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    'PlotCandle',\n",
    "]\n",
    "DROP_EXACT = {'Plot', 'Plot.1', 'Plot.2', 'div_bull_label', 'div_bear_label'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfad1a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Loading & Feature Layer Construction\n",
    "Helper functions to load a timeframe, clean columns, fill gaps, and add a few helper features (returns, Bollinger %B, Bollinger width). The result is the **feature layer** ready for labeling and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e318d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_filter_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    keep_cols = []\n",
    "    for col in df.columns:\n",
    "        if any(pattern in col for pattern in DROP_PATTERNS):\n",
    "            continue\n",
    "        if col in DROP_EXACT:\n",
    "            continue\n",
    "        keep_cols.append(col)\n",
    "    return df[keep_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539fb5e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Labeling (Binary Up/Down)\n",
    "Create a forward return label for a chosen horizon. By default we use 4 bars ahead. `y = 1` when the future return is positive, else `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_labels(df: pd.DataFrame, horizon: int = 4) -> pd.DataFrame:\n",
    "    future_price = df['close'].shift(-horizon)\n",
    "    df['future_return'] = future_price / df['close'] - 1\n",
    "    df['y'] = (df['future_return'] > 0).astype(int)\n",
    "    df = df.dropna(subset=['future_return'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba85abe",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Validation/Test Split (Time-Based)\n",
    "Use chronological splits (70/15/15). No shuffling is applied to respect temporal order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e668b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    X_train: pd.DataFrame\n",
    "    X_val: pd.DataFrame\n",
    "    X_test: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    y_val: pd.Series\n",
    "    y_test: pd.Series\n",
    "    future_val: pd.Series\n",
    "    future_test: pd.Series\n",
    "\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, feature_cols: List[str]) -> DatasetSplit:\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.7)\n",
    "    val_end = int(n * 0.85)\n",
    "\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end]\n",
    "    test = df.iloc[val_end:]\n",
    "\n",
    "    X_train, y_train = train[feature_cols], train['y']\n",
    "    X_val, y_val = val[feature_cols], val['y']\n",
    "    X_test, y_test = test[feature_cols], test['y']\n",
    "\n",
    "    return DatasetSplit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        future_val=val['future_return'],\n",
    "        future_test=test['future_return'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4b7f5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Modeling + Threshold Search\n",
    "Train a LightGBM classifier inside a scikit-learn pipeline (imputer + scaler). Validation Sharpe on the forward-return horizon is used to pick the best probability threshold for entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sharpe(returns: np.ndarray, periods_per_year: int = 365) -> float:\n",
    "    ret = np.array(returns)\n",
    "    if ret.std() == 0:\n",
    "        return 0.0\n",
    "    return (ret.mean() * periods_per_year) / (ret.std() * math.sqrt(periods_per_year))\n",
    "\n",
    "\n",
    "def evaluate_threshold(probs: np.ndarray, future_returns: pd.Series, candidate_thr: List[float]):\n",
    "    best_sharpe, best_thr = -np.inf, None\n",
    "    for thr in candidate_thr:\n",
    "        positions = (probs > thr).astype(int)\n",
    "        strat_ret = positions * future_returns.values\n",
    "        sharpe = compute_sharpe(strat_ret)\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe, best_thr = sharpe, thr\n",
    "    return best_thr, best_sharpe\n",
    "\n",
    "\n",
    "def train_model(split: DatasetSplit, feature_cols: List[str]):\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    imputer.fit(split.X_train)\n",
    "    scaler.fit(imputer.transform(split.X_train))\n",
    "\n",
    "    def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        arr = imputer.transform(df)\n",
    "        arr = scaler.transform(arr)\n",
    "        return pd.DataFrame(arr, columns=feature_cols, index=df.index)\n",
    "\n",
    "    X_train = transform(split.X_train)\n",
    "    X_val = transform(split.X_val)\n",
    "    X_test = transform(split.X_test)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"binary\",\n",
    "    )\n",
    "    model.fit(X_train, split.y_train)\n",
    "\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    candidate_thr = [0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "    best_thr, best_sharpe = evaluate_threshold(val_probs, split.future_val, candidate_thr)\n",
    "\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    positions_test = (test_probs > best_thr).astype(int)\n",
    "    strat_ret_test = positions_test * split.future_test.values\n",
    "\n",
    "    metrics = {\n",
    "        \"val_accuracy\": accuracy_score(split.y_val, (val_probs > 0.5).astype(int)),\n",
    "        \"test_accuracy\": accuracy_score(split.y_test, (test_probs > 0.5).astype(int)),\n",
    "        \"val_sharpe@best_thr\": best_sharpe,\n",
    "        \"test_sharpe@best_thr\": compute_sharpe(strat_ret_test),\n",
    "        \"threshold\": best_thr,\n",
    "    }\n",
    "\n",
    "    reports = {\n",
    "        \"val_report\": classification_report(split.y_val, (val_probs > 0.5).astype(int), digits=3),\n",
    "        \"test_report\": classification_report(split.y_test, (test_probs > 0.5).astype(int), digits=3),\n",
    "    }\n",
    "\n",
    "    processed = {\"train\": X_train, \"val\": X_val, \"test\": X_test}\n",
    "\n",
    "    return model, metrics, reports, val_probs, test_probs, processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941e999",
   "metadata": {},
   "source": [
    "\n",
    "## 7. End-to-End Runner (Per Timeframe)\n",
    "Combine all steps for a given timeframe and collect summary metrics. The primary focus is `1h`, with side-by-side stats for `4h` and `1d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79440a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_timeframe(key: str, horizon: int = 4):\n",
    "    df = build_feature_layer(DATA_PATHS[key])\n",
    "    df = add_labels(df, horizon=horizon)\n",
    "\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in {'time', 'y', 'future_return'}\n",
    "        and not col.startswith('div_')  # drop visual divergence labels\n",
    "    ]\n",
    "\n",
    "    split = time_based_split(df, feature_cols)\n",
    "    model, metrics, reports, val_probs, test_probs, processed = train_model(split, feature_cols)\n",
    "\n",
    "    summary = {\n",
    "        \"timeframe\": key,\n",
    "        \"rows\": len(df),\n",
    "        **metrics,\n",
    "    }\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"reports\": reports,\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"split\": split,\n",
    "        \"processed_features\": processed,\n",
    "        \"val_probs\": val_probs,\n",
    "        \"test_probs\": test_probs,\n",
    "    }\n",
    "\n",
    "results = {tf: run_timeframe(tf) for tf in DATA_PATHS}\n",
    "summary_df = pd.DataFrame([results[tf][\"summary\"] for tf in DATA_PATHS])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31c4b9",
   "metadata": {},
   "source": [
    "\n",
    "### Classification Reports (quick check)\n",
    "View precision/recall/F1 for each timeframe to understand base discrimination performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tf, res in results.items():\n",
    "    print(f\"=== {tf} ===\")\n",
    "    print(res['reports']['test_report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186adb",
   "metadata": {},
   "source": [
    "\n",
    "## 8. SHAP Explainability (Feature Importance)\n",
    "Use SHAP to see which indicators drive the `prob_up` predictions. Run on a subset for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if shap is not available in your environment\n",
    "# %pip install shap\n",
    "\n",
    "import io\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import warnings\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Suppress noisy LightGBM / SHAP output format warnings\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray',\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "IMG_DIR = Path('outputs/shap')\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError:\n",
    "    print(\"shap is not installed in this environment. Install with `pip install shap` to view explainability plots.\")\n",
    "else:\n",
    "    shap.initjs()\n",
    "\n",
    "    primary = results['1h']\n",
    "    X_explain = primary['processed_features']['test'].sample(\n",
    "        n=min(1000, len(primary['processed_features']['test'])), random_state=42\n",
    "    )\n",
    "    explainer = shap.TreeExplainer(primary['model'])\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values_class1 = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "    else:\n",
    "        shap_values_class1 = shap_values\n",
    "\n",
    "    def _save_and_display(fig, filename: str):\n",
    "        out_path = IMG_DIR / filename\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(out_path, bbox_inches='tight', dpi=150)\n",
    "        fig.savefig(buf, format='png', bbox_inches='tight', dpi=150)\n",
    "        buf.seek(0)\n",
    "        display(Image(data=buf.read(), format='png', embed=True))\n",
    "\n",
    "    try:\n",
    "        shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'], show=False)\n",
    "        _save_and_display(plt.gcf(), 'shap_beeswarm_1h.png')\n",
    "\n",
    "        shap.summary_plot(\n",
    "            shap_values_class1,\n",
    "            X_explain,\n",
    "            feature_names=primary['feature_cols'],\n",
    "            plot_type='bar',\n",
    "            show=False,\n",
    "        )\n",
    "        _save_and_display(plt.gcf(), 'shap_bar_1h.png')\n",
    "    except Exception as exc:\n",
    "        print(f\"SHAP plotting skipped: {exc}\")\n",
    "    finally:\n",
    "        plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59821548",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Next Steps & Risk Layer\n",
    "- Apply strategy-specific filters (reversal/trend/volatility) using the `prob_up` outputs and indicators (RSI, bb_percent, macd_hist, EMA stacks, ATR).\n",
    "- Add risk management: ATR-based SL/TP, max risk per trade, exposure caps.\n",
    "- Extend to multi-asset or orderbook factors when data is available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f1f56",
   "metadata": {},
   "source": [
    "## 10. How to read the outputs\n",
    "\n",
    "- **What the notebook gives you**\n",
    "  - `summary_df` (cell 7) → quick table of rows per timeframe plus Sharpe-like metric and accuracy stats for train/val/test. Use this to spot if a timeframe is overfitting (train >> val/test) or too noisy (all metrics ~0.5).\n",
    "  - Classification reports (cell 8) → precision/recall/F1 per class. Focus on `val` first; test should be close. If you see heavy imbalance, adjust thresholds or class weights.\n",
    "  - SHAP plots (cell 10) saved to files and displayed inline → top features driving the 1h model. Beeswarm shows directionality (red = high value, blue = low value). Bar plot ranks mean(|SHAP|) importance.\n",
    "\n",
    "- **How to check data/loading**\n",
    "  - Cell 3 prints a clear error if a CSV is missing and shows the resolved `PROJECT_ROOT`. If you run elsewhere, set `DATA_ROOT` env var to your folder containing the `BINANCE_BTCUSDT.P, *.csv` files.\n",
    "  - Ensure the `time` column parses correctly and the index is sorted; the helper already enforces this.\n",
    "\n",
    "- **How to interpret SHAP**\n",
    "  - Points on the **right** increase `prob_up`; on the **left** decrease it.\n",
    "  - Red cluster on the left for `rsi` means overbought reduces the up-probability; blue on the right means oversold supports upside.\n",
    "  - If a feature you expect (e.g., `trend_up` or `macd_hist`) is missing from top ranks, re-check column renaming or drop rules.\n",
    "\n",
    "- **What to do next (practical checks)**\n",
    "  - Re-run cells 7–10 after any data or feature change; keep an eye on validation Sharpe and SHAP ranking stability.\n",
    "  - If a timeframe underperforms, tweak `horizon`, add/remove features, or raise the prob threshold in cell 6’s search grid.\n",
    "  - Add simple fee/slippage assumptions in your backtest to see net performance.\n",
    "\n",
    "- **Save artifacts**\n",
    "  - Export trained models/scalers if you want to reuse them: `joblib.dump(model, 'outputs/lgbm_1h.pkl')` and keep the feature list from `primary['feature_cols']`.\n",
    "  - The SHAP images are saved in the working directory; move them to `outputs/` for record-keeping if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e11f08",
   "metadata": {},
   "source": [
    "\n",
    "## Strategi referensi & cara uji (indikator + ML)\n",
    "\n",
    "Gunakan tabel SHAP dan metrik validasi yang sudah ada untuk memilih strategi mana yang layak diujikan, lalu mapping ke logika berikut.\n",
    "\n",
    "### Cara praktis menjalankan uji strategi\n",
    "1. **Siapkan fitur wajib** per strategi di `feature_cols` (lihat daftar indikator di bawah). Jika ADX belum ada di CSV, hitung dulu dari OHLCV (bisa pakai `ta.trend.ADXIndicator`).\n",
    "2. **Pakai `results['<tf>']`** yang sudah berisi `train/val/test` + `prob_up`. Tambahkan kolom rule-based pada dataframe hasil sebelum backtest kecil:\n",
    "   ```python\n",
    "   df_tf = results['1h']['data']  # gunakan kunci 4h/1d sesuai kebutuhan\n",
    "   # contoh template entry/exit dengan prob_up + rule\n",
    "   df_tf['position'] = 0\n",
    "   long_mask = <rule_long>\n",
    "   short_mask = <rule_short>\n",
    "   df_tf.loc[long_mask, 'position'] = 1\n",
    "   df_tf.loc[short_mask, 'position'] = -1\n",
    "   ```\n",
    "   Setelah itu jalankan evaluator sederhana (PnL / Sharpe) seperti di sel validasi/test.\n",
    "3. **Tuning threshold**: iterasi `prob_up` cutoff (mis. 0.55–0.7 untuk long, 0.3–0.45 untuk short) dan filter indikator yang relevan, catat Sharpe/MaxDD per kombinasi.\n",
    "4. **Bandingkan lintas timeframe**: jalankan rule sama untuk `1h/4h/1d`, lalu pilih setup dengan trade frequency, Sharpe, dan drawdown paling sehat.\n",
    "\n",
    "### Ringkasan 10 strategi library\n",
    "- **(1) Reversal (sideways)** — butuh: `rsi`, `macd_hist`, `vwap`, `volume`, `bb_percent`, `bb_width`, `adx` (optional). Entry: `adx < 20`, long jika `prob_up > 0.6`, `rsi < 30`, `bb_percent < 0.1`; short jika `prob_up < 0.4`, `rsi > 70`, `bb_percent > 0.9`.\n",
    "- **(2) Trend-Following** — butuh: `ema_20`, `ema_50`, `ema_200`, `adx`, `volume`. Entry long: `adx > 25` & EMA berurutan naik & `prob_up > 0.55`; entry short: `adx > 25`, EMA berurutan turun, `prob_up < 0.45`; else `position = 0`.\n",
    "- **(3) Volatility Mean-Reversion** — butuh: `bb_percent`, `bb_width`, `atr`, `rsi`. Entry long: `bb_percent < 0.05`, `rsi < 25`, `bb_width` di atas median rolling 100, `prob_up > 0.65`; exit ide: `rsi > 50` atau `bb_percent > 0.5`.\n",
    "- **(4) Orderbook Momentum (advanced)** — butuh data orderbook: `imbalance`, `bid/ask volume spike`. Entry long: `imbalance > 0.6` & `prob_up > 0.7`; short: `prob_up < 0.3`. Pastikan efek latensi/slippage diperhitungkan di backtest.\n",
    "- **(5) Multi-Timeframe Ensemble** — gunakan `prob_up` per TF (`pred_1h`, `pred_4h`, `pred_1d`). Entry = 1 jika `pred_1h > 0.55` & `pred_4h > 0.60` & `pred_1d > 0.50`. Untuk versi stacking, fit meta `LogisticRegression` pada kolom prediksi.\n",
    "- **(6) Breakout & Vol Expansion** — butuh: `donchian_high/low` atau highest/lowest N-bar, `bb_width` atau `atr`, `volume`. Entry long: `close > donchian_high`, `bb_width > thr_vol`, volume > 1.5× MA50, `prob_up > 0.6`.\n",
    "- **(7) Time-Series Momentum + Vol Targeting** — butuh: rolling return (20/50), rolling std, `atr`. Entry long jika `mom_20 > quantile(0.7)` & `prob_up > 0.55`; leverage = `target_vol / realized_vol` (clip maks 3x), posisi = `position * leverage`.\n",
    "- **(8) Funding/Basis Carry (perp futures)** — butuh: `funding`, basis (perp−spot)/spot, tren EMA. Short perp jika `funding` di quantile 0.8, tren tidak turun kuat, `prob_up < 0.45`; long perp jika funding sangat negatif & `prob_up > 0.55`.\n",
    "- **(9) Cross-Sectional Momentum (multi-coin)** — butuh data multi-simbol. Hitung `mom_20` per coin, rank per waktu; long top-N, optional short bottom-N. Bisa ganti ranking berdasarkan `prob_up` model per coin.\n",
    "- **(10) Stat Arb / Pairs Trading** — butuh pasangan cointegrated (spread, z-score). Entry long A/short B jika `z < -2` & `prob_revert > 0.6`; sebaliknya untuk short A/long B.\n",
    "\n",
    "### Cara membaca hasil\n",
    "- **Feature importance (SHAP)**: gunakan top fitur untuk memutuskan filter strategi mana yang paling relevan; mis. jika `bb_percent` dan `rsi` dominan, prioritaskan strategi (1)/(3).\n",
    "- **Metrik validasi**: pilih strategi yang menjaga Sharpe/Sortino dan MaxDD setelah menerapkan rule di atas, bukan hanya AUC/accuracy.\n",
    "- **Artefak simpanan**: setelah menemukan kombinasi terbaik, simpan model, scaler, dan plot ke `outputs/indikator-ml-strategy/` agar dapat direpro.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
