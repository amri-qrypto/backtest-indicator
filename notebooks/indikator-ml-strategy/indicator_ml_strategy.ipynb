{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6725d40",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End Indicator ML Strategy (BTCUSDT)\n",
    "\n",
    "Blueprint notebook to transform the provided feature-rich CSVs into a clean feature layer, label future returns, train a baseline ML model, and backtest simple probability-to-position mappings. The main focus is the 1H dataset, with comparison against 4H and 1D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86444ff1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Paths\n",
    "Load core dependencies and configure the file locations for each timeframe. The 1H file is treated as the primary source while keeping 4H and 1D available for quick comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def discover_candidate_data_dirs() -> List[Path]:\n",
    "    \"\"\"Enumerate plausible data directories, including env overrides and repo root.\"\"\"\n",
    "    candidates: List[Path] = []\n",
    "    seen = set()\n",
    "\n",
    "    def add(path_like):\n",
    "        path = Path(path_like).resolve()\n",
    "        if path in seen:\n",
    "            return\n",
    "        seen.add(path)\n",
    "        candidates.append(path)\n",
    "\n",
    "    env_dir = os.getenv(\"DATA_DIR\")\n",
    "    if env_dir:\n",
    "        add(env_dir)\n",
    "\n",
    "    cwd = Path.cwd().resolve()\n",
    "    add(cwd / \"data\")\n",
    "    add(cwd / \"notebooks\" / \"data\")\n",
    "    for parent in cwd.parents:\n",
    "        add(parent / \"data\")\n",
    "        add(parent / \"notebooks\" / \"data\")\n",
    "\n",
    "    try:\n",
    "        repo_root = Path(\n",
    "            subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], text=True).strip()\n",
    "        )\n",
    "        add(repo_root / \"data\")\n",
    "        add(repo_root / \"notebooks\" / \"data\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return [p for p in candidates if p.exists()]\n",
    "\n",
    "\n",
    "def resolve_data_path(filename: str) -> Path:\n",
    "    data_dirs = discover_candidate_data_dirs()\n",
    "    attempts = []\n",
    "    for base in data_dirs:\n",
    "        candidate = base / filename\n",
    "        attempts.append(candidate)\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate data file '{filename}'. Checked: {attempts or ['<no data dirs found>']}. \"\n",
    "        \"Set DATA_DIR env var or place the file under one of the listed data directories.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_FILES = {\n",
    "    \"1h\": \"BINANCE_BTCUSDT.P, 60.csv\",\n",
    "    \"4h\": \"BINANCE_BTCUSDT.P, 240.csv\",\n",
    "    \"1d\": \"BINANCE_BTCUSDT.P, 1D.csv\",\n",
    "}\n",
    "DATA_PATHS = {tf: resolve_data_path(filename) for tf, filename in DATA_FILES.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878f7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_layer(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found: {path}. Checked candidate data dirs: {discover_candidate_data_dirs()}.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = rename_and_filter_columns(df)\n",
    "\n",
    "    # Basic cleaning\n",
    "    df = df.sort_values(\"time\").drop_duplicates(subset=[\"time\"])\n",
    "    df = df.set_index(\"time\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.ffill().bfill()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Helper features\n",
    "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
    "    df[\"bb_percent\"] = (df[\"close\"] - df[\"bb_lower\"]) / (df[\"bb_upper\"] - df[\"bb_lower\"])\n",
    "    df[\"bb_width\"] = (df[\"bb_upper\"] - df[\"bb_lower\"]) / df[\"bb_basis\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ae688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RENAME_MAP = {\n",
    "    'time': 'time',\n",
    "    'open': 'open',\n",
    "    'high': 'high',\n",
    "    'low': 'low',\n",
    "    'close': 'close',\n",
    "    'Volume': 'volume',\n",
    "    'VWAP': 'vwap',\n",
    "    'Upper Band #1': 'bb1_upper',\n",
    "    'Lower Band #1': 'bb1_lower',\n",
    "    'Basis': 'bb_basis',\n",
    "    'Upper': 'bb_upper',\n",
    "    'Lower': 'bb_lower',\n",
    "    'Up Trend': 'trend_up',\n",
    "    'Down Trend': 'trend_down',\n",
    "    'EMA': 'ema_fast',\n",
    "    'EMA.1': 'ema_slow',\n",
    "    'Conversion Line': 'ichi_conv',\n",
    "    'Base Line': 'ichi_base',\n",
    "    'Lagging Span': 'ichi_lag',\n",
    "    'Leading Span A': 'ichi_span_a',\n",
    "    'Leading Span B': 'ichi_span_b',\n",
    "    'Upper.1': 'channel_upper',\n",
    "    'Average': 'channel_mid',\n",
    "    'Lower.1': 'channel_lower',\n",
    "    'RSI': 'rsi',\n",
    "    'RSI-based MA': 'rsi_ma',\n",
    "    'Regular Bullish': 'div_bull',\n",
    "    'Regular Bullish Label': 'div_bull_label',\n",
    "    'Regular Bearish': 'div_bear',\n",
    "    'Regular Bearish Label': 'div_bear_label',\n",
    "    'Histogram': 'macd_hist',\n",
    "    'MACD': 'macd',\n",
    "    'Signal': 'macd_signal',\n",
    "    'ATR': 'atr',\n",
    "    'K': 'stoch_k',\n",
    "    'D': 'stoch_d',\n",
    "    '%R': 'williams_r',\n",
    "}\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    'PlotCandle',\n",
    "]\n",
    "DROP_EXACT = {'Plot', 'Plot.1', 'Plot.2', 'div_bull_label', 'div_bear_label'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfad1a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Loading & Feature Layer Construction\n",
    "Helper functions to load a timeframe, clean columns, fill gaps, and add a few helper features (returns, Bollinger %B, Bollinger width). The result is the **feature layer** ready for labeling and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e318d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rename_and_filter_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    keep_cols = []\n",
    "    for col in df.columns:\n",
    "        if any(pattern in col for pattern in DROP_PATTERNS):\n",
    "            continue\n",
    "        if col in DROP_EXACT:\n",
    "            continue\n",
    "        keep_cols.append(col)\n",
    "    df = df[keep_cols]\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_layer(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found: {path}. Resolved PROJECT_ROOT: {PROJECT_ROOT}.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = rename_and_filter_columns(df)\n",
    "\n",
    "    # Basic cleaning\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.sort_values(\"time\").drop_duplicates(subset=\"time\")\n",
    "    df = df.ffill().bfill()\n",
    "\n",
    "    # Derived helpers\n",
    "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
    "    df[\"bb_percent\"] = (df[\"close\"] - df[\"bb_lower\"]) / (df[\"bb_upper\"] - df[\"bb_lower\"])\n",
    "    df[\"bb_width\"] = (df[\"bb_upper\"] - df[\"bb_lower\"]) / df[\"bb_basis\"]\n",
    "\n",
    "    # Replace inf/nan from division\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539fb5e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Labeling (Binary Up/Down)\n",
    "Create a forward return label for a chosen horizon. By default we use 4 bars ahead. `y = 1` when the future return is positive, else `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6966425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_labels(df: pd.DataFrame, horizon: int = 4) -> pd.DataFrame:\n",
    "    future_price = df['close'].shift(-horizon)\n",
    "    df['future_return'] = future_price / df['close'] - 1\n",
    "    df['y'] = (df['future_return'] > 0).astype(int)\n",
    "    df = df.dropna(subset=['future_return'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba85abe",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Validation/Test Split (Time-Based)\n",
    "Use chronological splits (70/15/15). No shuffling is applied to respect temporal order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e668b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    X_train: pd.DataFrame\n",
    "    X_val: pd.DataFrame\n",
    "    X_test: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    y_val: pd.Series\n",
    "    y_test: pd.Series\n",
    "    future_val: pd.Series\n",
    "    future_test: pd.Series\n",
    "\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, feature_cols: List[str]) -> DatasetSplit:\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.7)\n",
    "    val_end = int(n * 0.85)\n",
    "\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end]\n",
    "    test = df.iloc[val_end:]\n",
    "\n",
    "    X_train, y_train = train[feature_cols], train['y']\n",
    "    X_val, y_val = val[feature_cols], val['y']\n",
    "    X_test, y_test = test[feature_cols], test['y']\n",
    "\n",
    "    return DatasetSplit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        future_val=val['future_return'],\n",
    "        future_test=test['future_return'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4b7f5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Modeling + Threshold Search\n",
    "Train a LightGBM classifier inside a scikit-learn pipeline (imputer + scaler). Validation Sharpe on the forward-return horizon is used to pick the best probability threshold for entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_sharpe(returns: np.ndarray, periods_per_year: int = 365) -> float:\n",
    "    ret = np.array(returns)\n",
    "    if ret.std() == 0:\n",
    "        return 0.0\n",
    "    return (ret.mean() * periods_per_year) / (ret.std() * math.sqrt(periods_per_year))\n",
    "\n",
    "\n",
    "def evaluate_threshold(probs: np.ndarray, future_returns: pd.Series, candidate_thr: List[float]):\n",
    "    best_sharpe, best_thr = -np.inf, None\n",
    "    for thr in candidate_thr:\n",
    "        positions = (probs > thr).astype(int)\n",
    "        strat_ret = positions * future_returns.values\n",
    "        sharpe = compute_sharpe(strat_ret)\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe, best_thr = sharpe, thr\n",
    "    return best_thr, best_sharpe\n",
    "\n",
    "\n",
    "def train_model(split: DatasetSplit, feature_cols: List[str]):\n",
    "    model = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\n",
    "            \"model\",\n",
    "            lgb.LGBMClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=-1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective=\"binary\",\n",
    "            ),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    model.fit(split.X_train, split.y_train)\n",
    "\n",
    "    val_probs = model.predict_proba(split.X_val)[:, 1]\n",
    "    candidate_thr = [0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "    best_thr, best_sharpe = evaluate_threshold(val_probs, split.future_val, candidate_thr)\n",
    "\n",
    "    test_probs = model.predict_proba(split.X_test)[:, 1]\n",
    "    positions_test = (test_probs > best_thr).astype(int)\n",
    "    strat_ret_test = positions_test * split.future_test.values\n",
    "\n",
    "    metrics = {\n",
    "        \"val_accuracy\": accuracy_score(split.y_val, (val_probs > 0.5).astype(int)),\n",
    "        \"test_accuracy\": accuracy_score(split.y_test, (test_probs > 0.5).astype(int)),\n",
    "        \"val_sharpe@best_thr\": best_sharpe,\n",
    "        \"test_sharpe@best_thr\": compute_sharpe(strat_ret_test),\n",
    "        \"threshold\": best_thr,\n",
    "    }\n",
    "\n",
    "    reports = {\n",
    "        \"val_report\": classification_report(split.y_val, (val_probs > 0.5).astype(int), digits=3),\n",
    "        \"test_report\": classification_report(split.y_test, (test_probs > 0.5).astype(int), digits=3),\n",
    "    }\n",
    "\n",
    "    return model, metrics, reports, val_probs, test_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941e999",
   "metadata": {},
   "source": [
    "\n",
    "## 7. End-to-End Runner (Per Timeframe)\n",
    "Combine all steps for a given timeframe and collect summary metrics. The primary focus is `1h`, with side-by-side stats for `4h` and `1d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79440a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9162, number of negative: 8666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8925\n",
      "[LightGBM] [Info] Number of data points in the train set: 17828, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.513911 -> initscore=0.055657\n",
      "[LightGBM] [Info] Start training from score 0.055657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jefri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4863, number of negative: 4675\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8925\n",
      "[LightGBM] [Info] Number of data points in the train set: 9538, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.509855 -> initscore=0.039426\n",
      "[LightGBM] [Info] Start training from score 0.039426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jefri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 844, number of negative: 744\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8691\n",
      "[LightGBM] [Info] Number of data points in the train set: 1588, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.531486 -> initscore=0.126111\n",
      "[LightGBM] [Info] Start training from score 0.126111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jefri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeframe</th>\n",
       "      <th>rows</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>val_sharpe@best_thr</th>\n",
       "      <th>test_sharpe@best_thr</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1h</td>\n",
       "      <td>25469</td>\n",
       "      <td>0.549738</td>\n",
       "      <td>0.518451</td>\n",
       "      <td>1.031942</td>\n",
       "      <td>0.189487</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4h</td>\n",
       "      <td>13627</td>\n",
       "      <td>0.524462</td>\n",
       "      <td>0.527139</td>\n",
       "      <td>1.793066</td>\n",
       "      <td>0.627287</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d</td>\n",
       "      <td>2269</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>3.531258</td>\n",
       "      <td>-0.113110</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timeframe   rows  val_accuracy  test_accuracy  val_sharpe@best_thr  \\\n",
       "0        1h  25469      0.549738       0.518451             1.031942   \n",
       "1        4h  13627      0.524462       0.527139             1.793066   \n",
       "2        1d   2269      0.494118       0.483871             3.531258   \n",
       "\n",
       "   test_sharpe@best_thr  threshold  \n",
       "0              0.189487        0.5  \n",
       "1              0.627287        0.5  \n",
       "2             -0.113110        0.5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def run_timeframe(key: str, horizon: int = 4):\n",
    "    df = build_feature_layer(DATA_PATHS[key])\n",
    "    df = add_labels(df, horizon=horizon)\n",
    "\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in {'time', 'y', 'future_return'}\n",
    "        and not col.startswith('div_')  # drop visual divergence labels\n",
    "    ]\n",
    "\n",
    "    split = time_based_split(df, feature_cols)\n",
    "    model, metrics, reports, val_probs, test_probs = train_model(split, feature_cols)\n",
    "\n",
    "    summary = {\n",
    "        \"timeframe\": key,\n",
    "        \"rows\": len(df),\n",
    "        **metrics,\n",
    "    }\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"reports\": reports,\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"split\": split,\n",
    "        \"val_probs\": val_probs,\n",
    "        \"test_probs\": test_probs,\n",
    "    }\n",
    "\n",
    "results = {tf: run_timeframe(tf) for tf in DATA_PATHS}\n",
    "summary_df = pd.DataFrame([results[tf][\"summary\"] for tf in DATA_PATHS])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31c4b9",
   "metadata": {},
   "source": [
    "\n",
    "### Classification Reports (quick check)\n",
    "View precision/recall/F1 for each timeframe to understand base discrimination performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6158ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1h ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.526     0.409     0.460      1916\n",
      "           1      0.514     0.629     0.566      1905\n",
      "\n",
      "    accuracy                          0.518      3821\n",
      "   macro avg      0.520     0.519     0.513      3821\n",
      "weighted avg      0.520     0.518     0.513      3821\n",
      "\n",
      "=== 4h ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.512     0.785     0.619      1003\n",
      "           1      0.574     0.279     0.376      1042\n",
      "\n",
      "    accuracy                          0.527      2045\n",
      "   macro avg      0.543     0.532     0.498      2045\n",
      "weighted avg      0.543     0.527     0.495      2045\n",
      "\n",
      "=== 1d ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.478     0.957     0.638       162\n",
      "           1      0.588     0.056     0.102       179\n",
      "\n",
      "    accuracy                          0.484       341\n",
      "   macro avg      0.533     0.506     0.370       341\n",
      "weighted avg      0.536     0.484     0.357       341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tf, res in results.items():\n",
    "    print(f\"=== {tf} ===\")\n",
    "    print(res['reports']['test_report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186adb",
   "metadata": {},
   "source": [
    "\n",
    "## 8. SHAP Explainability (Feature Importance)\n",
    "Use SHAP to see which indicators drive the `prob_up` predictions. Run on a subset for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d536fced",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Uncomment if shap is not available in your environment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# %pip install shap\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      5\u001b[0m shap\u001b[38;5;241m.\u001b[39minitjs()\n\u001b[0;32m      7\u001b[0m primary \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1h\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Uncomment if shap is not available in your environment\n",
    "# %pip install shap\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "primary = results['1h']\n",
    "X_explain = primary['split'].X_test.sample(n=min(1000, len(primary['split'].X_test)), random_state=42)\n",
    "explainer = shap.TreeExplainer(primary['model'].named_steps['model'])\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_class1 = shap_values[1]\n",
    "else:\n",
    "    shap_values_class1 = shap_values\n",
    "\n",
    "shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'])\n",
    "shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'], plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59821548",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Next Steps & Risk Layer\n",
    "- Apply strategy-specific filters (reversal/trend/volatility) using the `prob_up` outputs and indicators (RSI, bb_percent, macd_hist, EMA stacks, ATR).\n",
    "- Add risk management: ATR-based SL/TP, max risk per trade, exposure caps.\n",
    "- Extend to multi-asset or orderbook factors when data is available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
