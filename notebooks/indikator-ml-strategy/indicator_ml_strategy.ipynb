{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6725d40",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End Indicator ML Strategy (BTCUSDT)\n",
    "\n",
    "Blueprint notebook to transform the provided feature-rich CSVs into a clean feature layer, label future returns, train a baseline ML model, and backtest simple probability-to-position mappings. The main focus is the 1H dataset, with comparison against 4H and 1D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86444ff1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Paths\n",
    "Load core dependencies and configure the file locations for each timeframe. The 1H file is treated as the primary source while keeping 4H and 1D available for quick comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def discover_candidate_data_dirs() -> List[Path]:\n",
    "    \"\"\"Enumerate plausible data directories, including env overrides and repo root.\"\"\"\n",
    "    candidates: List[Path] = []\n",
    "    seen = set()\n",
    "\n",
    "    def add(path_like):\n",
    "        path = Path(path_like).resolve()\n",
    "        if path in seen:\n",
    "            return\n",
    "        seen.add(path)\n",
    "        candidates.append(path)\n",
    "\n",
    "    env_dir = os.getenv(\"DATA_DIR\")\n",
    "    if env_dir:\n",
    "        add(env_dir)\n",
    "\n",
    "    cwd = Path.cwd().resolve()\n",
    "    add(cwd / \"data\")\n",
    "    add(cwd / \"notebooks\" / \"data\")\n",
    "    for parent in cwd.parents:\n",
    "        add(parent / \"data\")\n",
    "        add(parent / \"notebooks\" / \"data\")\n",
    "\n",
    "    try:\n",
    "        repo_root = Path(\n",
    "            subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], text=True).strip()\n",
    "        )\n",
    "        add(repo_root / \"data\")\n",
    "        add(repo_root / \"notebooks\" / \"data\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return [p for p in candidates if p.exists()]\n",
    "\n",
    "\n",
    "def resolve_data_path(filename: str) -> Path:\n",
    "    data_dirs = discover_candidate_data_dirs()\n",
    "    attempts = []\n",
    "    for base in data_dirs:\n",
    "        candidate = base / filename\n",
    "        attempts.append(candidate)\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate data file '{filename}'. Checked: {attempts or ['<no data dirs found>']}. \"\n",
    "        \"Set DATA_DIR env var or place the file under one of the listed data directories.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_FILES = {\n",
    "    \"1h\": \"BINANCE_BTCUSDT.P, 60.csv\",\n",
    "    \"4h\": \"BINANCE_BTCUSDT.P, 240.csv\",\n",
    "    \"1d\": \"BINANCE_BTCUSDT.P, 1D.csv\",\n",
    "}\n",
    "DATA_PATHS = {tf: resolve_data_path(filename) for tf, filename in DATA_FILES.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878f7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_layer(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found: {path}. Checked candidate data dirs: {discover_candidate_data_dirs()}.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = rename_and_filter_columns(df)\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.sort_values(\"time\").drop_duplicates(subset=[\"time\"])\n",
    "    df = df.set_index(\"time\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.ffill().bfill()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
    "    df[\"bb_percent\"] = (df[\"close\"] - df[\"bb_lower\"]) / (df[\"bb_upper\"] - df[\"bb_lower\"])\n",
    "    df[\"bb_width\"] = (df[\"bb_upper\"] - df[\"bb_lower\"]) / df[\"bb_basis\"]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ae688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RENAME_MAP = {\n",
    "    'time': 'time',\n",
    "    'open': 'open',\n",
    "    'high': 'high',\n",
    "    'low': 'low',\n",
    "    'close': 'close',\n",
    "    'Volume': 'volume',\n",
    "    'VWAP': 'vwap',\n",
    "    'Upper Band #1': 'bb1_upper',\n",
    "    'Lower Band #1': 'bb1_lower',\n",
    "    'Basis': 'bb_basis',\n",
    "    'Upper': 'bb_upper',\n",
    "    'Lower': 'bb_lower',\n",
    "    'Up Trend': 'trend_up',\n",
    "    'Down Trend': 'trend_down',\n",
    "    'EMA': 'ema_fast',\n",
    "    'EMA.1': 'ema_slow',\n",
    "    'Conversion Line': 'ichi_conv',\n",
    "    'Base Line': 'ichi_base',\n",
    "    'Lagging Span': 'ichi_lag',\n",
    "    'Leading Span A': 'ichi_span_a',\n",
    "    'Leading Span B': 'ichi_span_b',\n",
    "    'Upper.1': 'channel_upper',\n",
    "    'Average': 'channel_mid',\n",
    "    'Lower.1': 'channel_lower',\n",
    "    'RSI': 'rsi',\n",
    "    'RSI-based MA': 'rsi_ma',\n",
    "    'Regular Bullish': 'div_bull',\n",
    "    'Regular Bullish Label': 'div_bull_label',\n",
    "    'Regular Bearish': 'div_bear',\n",
    "    'Regular Bearish Label': 'div_bear_label',\n",
    "    'Histogram': 'macd_hist',\n",
    "    'MACD': 'macd',\n",
    "    'Signal': 'macd_signal',\n",
    "    'ATR': 'atr',\n",
    "    'K': 'stoch_k',\n",
    "    'D': 'stoch_d',\n",
    "    '%R': 'williams_r',\n",
    "}\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    'PlotCandle',\n",
    "]\n",
    "DROP_EXACT = {'Plot', 'Plot.1', 'Plot.2', 'div_bull_label', 'div_bear_label'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfad1a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Loading & Feature Layer Construction\n",
    "Helper functions to load a timeframe, clean columns, fill gaps, and add a few helper features (returns, Bollinger %B, Bollinger width). The result is the **feature layer** ready for labeling and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e318d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_filter_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    keep_cols = []\n",
    "    for col in df.columns:\n",
    "        if any(pattern in col for pattern in DROP_PATTERNS):\n",
    "            continue\n",
    "        if col in DROP_EXACT:\n",
    "            continue\n",
    "        keep_cols.append(col)\n",
    "    return df[keep_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539fb5e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Labeling (Binary Up/Down)\n",
    "Create a forward return label for a chosen horizon. By default we use 4 bars ahead. `y = 1` when the future return is positive, else `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6966425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_labels(df: pd.DataFrame, horizon: int = 4) -> pd.DataFrame:\n",
    "    future_price = df['close'].shift(-horizon)\n",
    "    df['future_return'] = future_price / df['close'] - 1\n",
    "    df['y'] = (df['future_return'] > 0).astype(int)\n",
    "    df = df.dropna(subset=['future_return'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba85abe",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Validation/Test Split (Time-Based)\n",
    "Use chronological splits (70/15/15). No shuffling is applied to respect temporal order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e668b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    X_train: pd.DataFrame\n",
    "    X_val: pd.DataFrame\n",
    "    X_test: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    y_val: pd.Series\n",
    "    y_test: pd.Series\n",
    "    future_val: pd.Series\n",
    "    future_test: pd.Series\n",
    "\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, feature_cols: List[str]) -> DatasetSplit:\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.7)\n",
    "    val_end = int(n * 0.85)\n",
    "\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end]\n",
    "    test = df.iloc[val_end:]\n",
    "\n",
    "    X_train, y_train = train[feature_cols], train['y']\n",
    "    X_val, y_val = val[feature_cols], val['y']\n",
    "    X_test, y_test = test[feature_cols], test['y']\n",
    "\n",
    "    return DatasetSplit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        future_val=val['future_return'],\n",
    "        future_test=test['future_return'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4b7f5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Modeling + Threshold Search\n",
    "Train a LightGBM classifier inside a scikit-learn pipeline (imputer + scaler). Validation Sharpe on the forward-return horizon is used to pick the best probability threshold for entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sharpe(returns: np.ndarray, periods_per_year: int = 365) -> float:\n",
    "    ret = np.array(returns)\n",
    "    if ret.std() == 0:\n",
    "        return 0.0\n",
    "    return (ret.mean() * periods_per_year) / (ret.std() * math.sqrt(periods_per_year))\n",
    "\n",
    "\n",
    "def compute_sortino(returns: np.ndarray, periods_per_year: int = 365) -> float:\n",
    "    ret = np.array(returns)\n",
    "    downside = ret[ret < 0]\n",
    "    if downside.std() == 0:\n",
    "        return 0.0\n",
    "    return (ret.mean() * periods_per_year) / (downside.std() * math.sqrt(periods_per_year))\n",
    "\n",
    "\n",
    "def evaluate_threshold(probs: np.ndarray, future_returns: pd.Series, candidate_thr: List[float]):\n",
    "    best_sharpe, best_thr = -np.inf, None\n",
    "    for thr in candidate_thr:\n",
    "        positions = (probs > thr).astype(int)\n",
    "        strat_ret = positions * future_returns.values\n",
    "        sharpe = compute_sharpe(strat_ret)\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe, best_thr = sharpe, thr\n",
    "    return best_thr, best_sharpe\n",
    "\n",
    "\n",
    "def train_model(split: DatasetSplit, feature_cols: List[str]):\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    imputer.fit(split.X_train)\n",
    "    scaler.fit(imputer.transform(split.X_train))\n",
    "\n",
    "    def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        arr = imputer.transform(df)\n",
    "        arr = scaler.transform(arr)\n",
    "        return pd.DataFrame(arr, columns=feature_cols, index=df.index)\n",
    "\n",
    "    X_train = transform(split.X_train)\n",
    "    X_val = transform(split.X_val)\n",
    "    X_test = transform(split.X_test)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"binary\",\n",
    "    )\n",
    "    model.fit(X_train, split.y_train)\n",
    "\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    candidate_thr = [0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "    best_thr, best_sharpe = evaluate_threshold(val_probs, split.future_val, candidate_thr)\n",
    "\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    positions_test = (test_probs > best_thr).astype(int)\n",
    "    strat_ret_test = positions_test * split.future_test.values\n",
    "\n",
    "    metrics = {\n",
    "        \"val_accuracy\": accuracy_score(split.y_val, (val_probs > 0.5).astype(int)),\n",
    "        \"test_accuracy\": accuracy_score(split.y_test, (test_probs > 0.5).astype(int)),\n",
    "        \"val_sharpe@best_thr\": best_sharpe,\n",
    "        \"test_sharpe@best_thr\": compute_sharpe(strat_ret_test),\n",
    "        \"threshold\": best_thr,\n",
    "    }\n",
    "\n",
    "    reports = {\n",
    "        \"val_report\": classification_report(split.y_val, (val_probs > 0.5).astype(int), digits=3),\n",
    "        \"test_report\": classification_report(split.y_test, (test_probs > 0.5).astype(int), digits=3),\n",
    "    }\n",
    "\n",
    "    processed = {\"train\": X_train, \"val\": X_val, \"test\": X_test}\n",
    "    preprocess = {\"imputer\": imputer, \"scaler\": scaler}\n",
    "\n",
    "    return model, metrics, reports, val_probs, test_probs, processed, preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb3088",
   "metadata": {},
   "source": [
    "### Extended validation/test metrics (AUC & PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79440a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_timeframe(key: str, horizon: int = 4):\n",
    "    df = build_feature_layer(DATA_PATHS[key])\n",
    "    df = add_labels(df, horizon=horizon)\n",
    "\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in {'time', 'y', 'future_return'}\n",
    "        and not col.startswith('div_')  # drop visual divergence labels\n",
    "    ]\n",
    "\n",
    "    split = time_based_split(df, feature_cols)\n",
    "    model, metrics, reports, val_probs, test_probs, processed, preprocess = train_model(split, feature_cols)\n",
    "\n",
    "    summary = {\n",
    "        \"timeframe\": key,\n",
    "        \"rows\": len(df),\n",
    "        **metrics,\n",
    "    }\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"reports\": reports,\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"split\": split,\n",
    "        \"processed_features\": processed,\n",
    "        \"preprocessing\": preprocess,\n",
    "        \"raw_df\": df,\n",
    "        \"val_probs\": val_probs,\n",
    "        \"test_probs\": test_probs,\n",
    "    }\n",
    "\n",
    "results = {tf: run_timeframe(tf) for tf in DATA_PATHS}\n",
    "summary_df = pd.DataFrame([results[tf][\"summary\"] for tf in DATA_PATHS])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rows = []\n",
    "for tf, res in results.items():\n",
    "    split = res['split']\n",
    "    metrics_rows.append({\n",
    "        \"timeframe\": tf,\n",
    "        \"val_auc\": roc_auc_score(split.y_val, res['val_probs']),\n",
    "        \"val_pr\": average_precision_score(split.y_val, res['val_probs']),\n",
    "        \"test_auc\": roc_auc_score(split.y_test, res['test_probs']),\n",
    "        \"test_pr\": average_precision_score(split.y_test, res['test_probs']),\n",
    "        \"val_sharpe@best_thr\": res['summary']['val_sharpe@best_thr'],\n",
    "        \"test_sharpe@best_thr\": res['summary']['test_sharpe@best_thr'],\n",
    "    })\n",
    "\n",
    "metrics_table = pd.DataFrame(metrics_rows).set_index('timeframe')\n",
    "metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3ec05",
   "metadata": {},
   "source": [
    "## 6a. Strategy metrics + backtest helpers\n",
    "Utilities for Sortino, drawdown, and risk-aware backtesting with optional feature-based filters (trend, Bollinger %B, stochastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_drawdown(equity_curve):\n",
    "    peak = -np.inf\n",
    "    drawdowns = []\n",
    "    for value in equity_curve:\n",
    "        peak = max(peak, value)\n",
    "        drawdowns.append((value - peak) / peak if peak != 0 else 0)\n",
    "    return drawdowns\n",
    "\n",
    "\n",
    "def apply_filter_mask(probs: pd.Series, features: pd.DataFrame, threshold: float, filter_cfg: Dict) -> pd.Series:\n",
    "    mask = probs > threshold\n",
    "\n",
    "    if filter_cfg.get(\"trend\") == \"up\" and \"trend_up\" in features and \"trend_down\" in features:\n",
    "        mask &= features[\"trend_up\"] > features[\"trend_down\"]\n",
    "    elif filter_cfg.get(\"trend\") == \"down\" and \"trend_up\" in features and \"trend_down\" in features:\n",
    "        mask &= features[\"trend_down\"] > features[\"trend_up\"]\n",
    "\n",
    "    bb_q = filter_cfg.get(\"bb_quantile\")\n",
    "    if bb_q is not None and \"bb_percent\" in features:\n",
    "        cutoff = features[\"bb_percent\"].quantile(bb_q)\n",
    "        if filter_cfg.get(\"bb_side\", \"low\") == \"low\":\n",
    "            mask &= features[\"bb_percent\"] <= cutoff\n",
    "        else:\n",
    "            mask &= features[\"bb_percent\"] >= cutoff\n",
    "\n",
    "    stoch_q = filter_cfg.get(\"stoch_quantile\")\n",
    "    if stoch_q is not None and \"stoch_k\" in features:\n",
    "        cutoff = features[\"stoch_k\"].quantile(stoch_q)\n",
    "        if filter_cfg.get(\"stoch_side\", \"low\") == \"low\":\n",
    "            mask &= features[\"stoch_k\"] <= cutoff\n",
    "        else:\n",
    "            mask &= features[\"stoch_k\"] >= cutoff\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def run_backtest_with_risk(\n",
    "    future_returns: pd.Series,\n",
    "    probs: pd.Series,\n",
    "    features: pd.DataFrame,\n",
    "    threshold: float,\n",
    "    filter_cfg: Dict,\n",
    "    fee: float = 0.0004,\n",
    "    slippage: float = 0.0005,\n",
    "    risk_per_trade: float = 0.01,\n",
    "    atr_mult: float = 1.5,\n",
    "    max_position: float = 3.0,\n",
    "    initial_equity: float = 1_000.0,\n",
    "):\n",
    "    features = features.reset_index(drop=True)\n",
    "    future_returns = pd.Series(future_returns).reset_index(drop=True)\n",
    "    probs = pd.Series(probs).reset_index(drop=True)\n",
    "\n",
    "    entries = apply_filter_mask(probs, features, threshold, filter_cfg)\n",
    "\n",
    "    equity = initial_equity\n",
    "    peak = initial_equity\n",
    "    equity_curve, exposure_curve = [], []\n",
    "    trade_returns, realized_returns = [], []\n",
    "    wins = 0\n",
    "\n",
    "    cost = 2 * (fee + slippage)\n",
    "\n",
    "    for take, ret, (_, row) in zip(entries, future_returns, features.iterrows()):\n",
    "        atr = row.get(\"atr\", np.nan)\n",
    "        close = row.get(\"close\", np.nan)\n",
    "        atr_pct = 0 if pd.isna(atr) or pd.isna(close) or close == 0 else atr / close\n",
    "        size_frac = risk_per_trade / max(atr_pct * atr_mult, 1e-8)\n",
    "        size_frac = min(size_frac, max_position)\n",
    "\n",
    "        net_ret = (ret if pd.notna(ret) else 0.0) - cost\n",
    "        realized = size_frac * net_ret if take else 0.0\n",
    "\n",
    "        equity *= 1 + realized\n",
    "        peak = max(peak, equity)\n",
    "        equity_curve.append(equity)\n",
    "        exposure_curve.append(size_frac if take else 0.0)\n",
    "        realized_returns.append(realized)\n",
    "\n",
    "        if take:\n",
    "            trade_returns.append(realized)\n",
    "            wins += net_ret > 0\n",
    "\n",
    "    drawdown_curve = compute_max_drawdown(equity_curve)\n",
    "    trades = max(len(trade_returns), 1)\n",
    "    metrics = {\n",
    "        \"filter\": filter_cfg.get(\"name\", \"custom\"),\n",
    "        \"threshold\": threshold,\n",
    "        \"sharpe\": compute_sharpe(realized_returns),\n",
    "        \"sortino\": compute_sortino(realized_returns),\n",
    "        \"max_drawdown\": abs(min(drawdown_curve)) if drawdown_curve else 0.0,\n",
    "        \"win_rate\": wins / trades,\n",
    "        \"trades\": trades,\n",
    "        \"total_return\": (equity_curve[-1] / initial_equity - 1) if equity_curve else 0.0,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"equity_curve\": equity_curve,\n",
    "        \"drawdown_curve\": drawdown_curve,\n",
    "        \"exposure_curve\": exposure_curve,\n",
    "        \"metrics\": metrics,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941e999",
   "metadata": {},
   "source": [
    "\n",
    "## 7. End-to-End Runner (Per Timeframe)\n",
    "Combine all steps for a given timeframe and collect summary metrics. The primary focus is `1h`, with side-by-side stats for `4h` and `1d`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31c4b9",
   "metadata": {},
   "source": [
    "\n",
    "### Classification Reports (quick check)\n",
    "View precision/recall/F1 for each timeframe to understand base discrimination performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tf, res in results.items():\n",
    "    print(f\"=== {tf} ===\")\n",
    "    print(res['reports']['test_report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186adb",
   "metadata": {},
   "source": [
    "\n",
    "## 8. SHAP Explainability (Feature Importance)\n",
    "Use SHAP to see which indicators drive the `prob_up` predictions. Run on a subset for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if shap is not available in your environment\n",
    "# %pip install shap\n",
    "\n",
    "import io\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import warnings\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Suppress noisy LightGBM / SHAP output format warnings\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray',\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "IMG_DIR = Path('outputs/shap')\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError:\n",
    "    print(\"shap is not installed in this environment. Install with `pip install shap` to view explainability plots.\")\n",
    "else:\n",
    "    shap.initjs()\n",
    "\n",
    "    primary = results['1h']\n",
    "    X_explain = primary['processed_features']['test'].sample(\n",
    "        n=min(1000, len(primary['processed_features']['test'])), random_state=42\n",
    "    )\n",
    "    explainer = shap.TreeExplainer(primary['model'])\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values_class1 = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "    else:\n",
    "        shap_values_class1 = shap_values\n",
    "\n",
    "    def _save_and_display(fig, filename: str):\n",
    "        out_path = IMG_DIR / filename\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(out_path, bbox_inches='tight', dpi=150)\n",
    "        fig.savefig(buf, format='png', bbox_inches='tight', dpi=150)\n",
    "        buf.seek(0)\n",
    "        display(Image(data=buf.read(), format='png', embed=True))\n",
    "\n",
    "    try:\n",
    "        shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'], show=False)\n",
    "        _save_and_display(plt.gcf(), 'shap_beeswarm_1h.png')\n",
    "\n",
    "        shap.summary_plot(\n",
    "            shap_values_class1,\n",
    "            X_explain,\n",
    "            feature_names=primary['feature_cols'],\n",
    "            plot_type='bar',\n",
    "            show=False,\n",
    "        )\n",
    "        _save_and_display(plt.gcf(), 'shap_bar_1h.png')\n",
    "    except Exception as exc:\n",
    "        print(f\"SHAP plotting skipped: {exc}\")\n",
    "    finally:\n",
    "        plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a60780",
   "metadata": {},
   "source": [
    "## 9. Multi-timeframe SHAP ranking\n",
    "Aggregate mean(|SHAP|) per feature across 1h/4h/1d and visualize the ranking heatmap plus top-10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'shap' not in globals():\n",
    "    print('SHAP not available; install shap to compute rankings.')\n",
    "else:\n",
    "    shap_importances = {}\n",
    "    shap_samples = {}\n",
    "    for tf, res in results.items():\n",
    "        X_explain = res['processed_features']['test'].sample(\n",
    "            n=min(800, len(res['processed_features']['test'])), random_state=42\n",
    "        )\n",
    "        explainer = shap.TreeExplainer(res['model'])\n",
    "        shap_vals = explainer.shap_values(X_explain)\n",
    "        if isinstance(shap_vals, list):\n",
    "            shap_vals = shap_vals[1] if len(shap_vals) > 1 else shap_vals[0]\n",
    "        shap_vals = np.array(shap_vals)\n",
    "        shap_samples[tf] = shap_vals\n",
    "        importance = pd.Series(np.abs(shap_vals).mean(axis=0), index=res['feature_cols'])\n",
    "        shap_importances[tf] = importance.sort_values(ascending=False)\n",
    "\n",
    "    shap_df = pd.DataFrame(shap_importances).fillna(0)\n",
    "    rank_df = shap_df.rank(ascending=False, method='dense')\n",
    "    rank_df['global_rank'] = rank_df.mean(axis=1)\n",
    "    rank_df = rank_df.sort_values('global_rank')\n",
    "\n",
    "    top10 = shap_df.mean(axis=1).sort_values(ascending=False).head(10)\n",
    "    display(top10)\n",
    "\n",
    "    top_features = rank_df.head(15).index\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(rank_df.loc[top_features, ['1h', '4h', '1d']], annot=True, fmt='.0f', cmap='mako_r')\n",
    "    plt.title('Feature ranking (lower is stronger)')\n",
    "    heatmap_path = IMG_DIR / 'shap_rank_heatmap.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(heatmap_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved SHAP ranking heatmap to {heatmap_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fab7a9",
   "metadata": {},
   "source": [
    "## 10. Validation grid: threshold + feature-aware filters\n",
    "Search probability thresholds and feature-based filters (trend, Bollinger %B, stochastic) using Sharpe/Sortino and drawdown-aware metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_grid = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75]\n",
    "filter_grid = [\n",
    "    {\"name\": \"prob_only\"},\n",
    "    {\"name\": \"trend_up\", \"trend\": \"up\"},\n",
    "    {\"name\": \"trend_down\", \"trend\": \"down\"},\n",
    "    {\"name\": \"bb_low_stoch_low\", \"bb_quantile\": 0.2, \"bb_side\": \"low\", \"stoch_quantile\": 0.25, \"stoch_side\": \"low\"},\n",
    "    {\"name\": \"bb_high_trend_up\", \"bb_quantile\": 0.8, \"bb_side\": \"high\", \"trend\": \"up\"},\n",
    "]\n",
    "\n",
    "primary = results['1h']\n",
    "val_features = primary['split'].X_val.reset_index(drop=True)\n",
    "val_future = primary['split'].future_val.reset_index(drop=True)\n",
    "val_probs = pd.Series(primary['val_probs']).reset_index(drop=True)\n",
    "\n",
    "strategy_rows = []\n",
    "for thr in threshold_grid:\n",
    "    for cfg in filter_grid:\n",
    "        bt = run_backtest_with_risk(\n",
    "            future_returns=val_future,\n",
    "            probs=val_probs,\n",
    "            features=val_features,\n",
    "            threshold=thr,\n",
    "            filter_cfg=cfg,\n",
    "            fee=0.0004,\n",
    "            slippage=0.0005,\n",
    "            risk_per_trade=0.01,\n",
    "            atr_mult=1.5,\n",
    "        )\n",
    "        row = dict(cfg)\n",
    "        row['threshold'] = thr\n",
    "        row.update(bt['metrics'])\n",
    "        strategy_rows.append(row)\n",
    "\n",
    "strategy_df = pd.DataFrame(strategy_rows)\n",
    "strategy_df = strategy_df.sort_values(['sharpe', 'sortino'], ascending=False)\n",
    "strategy_df.head(10)\n",
    "\n",
    "best_config = strategy_df.iloc[0]\n",
    "best_filter_cfg = next(cfg for cfg in filter_grid if cfg['name'] == best_config['filter'])\n",
    "print('Selected config:', best_config[['filter', 'threshold', 'sharpe', 'sortino', 'max_drawdown', 'win_rate']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b6519",
   "metadata": {},
   "source": [
    "## 11. Timeframe stability check\n",
    "Compare AUC/PR plus top SHAP drivers to see when each timeframe is preferable or combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e125224",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'shap_importances' in globals():\n",
    "    shap_top5 = {\n",
    "        tf: imp.sort_values(ascending=False).head(5).index.tolist()\n",
    "        for tf, imp in shap_importances.items()\n",
    "    }\n",
    "else:\n",
    "    shap_top5 = {tf: [] for tf in results}\n",
    "\n",
    "stability_rows = []\n",
    "for tf, res in results.items():\n",
    "    split = res['split']\n",
    "    stability_rows.append({\n",
    "        'timeframe': tf,\n",
    "        'val_auc': roc_auc_score(split.y_val, res['val_probs']),\n",
    "        'test_auc': roc_auc_score(split.y_test, res['test_probs']),\n",
    "        'val_pr': average_precision_score(split.y_val, res['val_probs']),\n",
    "        'test_pr': average_precision_score(split.y_test, res['test_probs']),\n",
    "        'val_sharpe@best_thr': res['summary']['val_sharpe@best_thr'],\n",
    "        'test_sharpe@best_thr': res['summary']['test_sharpe@best_thr'],\n",
    "        'top5_shap': ', '.join(shap_top5.get(tf, [])),\n",
    "    })\n",
    "\n",
    "stability_table = pd.DataFrame(stability_rows).set_index('timeframe')\n",
    "stability_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712d679",
   "metadata": {},
   "source": [
    "## 12. Realistic backtest with fee/slippage + ATR sizing\n",
    "Apply the best validation config on the 1h test split with ATR-based sizing, and visualize equity, exposure, and drawdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fa809",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = primary['split'].X_test.reset_index(drop=True)\n",
    "test_future = primary['split'].future_test.reset_index(drop=True)\n",
    "test_probs = pd.Series(primary['test_probs']).reset_index(drop=True)\n",
    "\n",
    "best_bt = run_backtest_with_risk(\n",
    "    future_returns=test_future,\n",
    "    probs=test_probs,\n",
    "    features=test_features,\n",
    "    threshold=float(best_config['threshold']),\n",
    "    filter_cfg=dict(best_filter_cfg),\n",
    "    fee=0.0004,\n",
    "    slippage=0.0005,\n",
    "    risk_per_trade=0.01,\n",
    "    atr_mult=1.5,\n",
    ")\n",
    "\n",
    "best_metrics = pd.Series(best_bt['metrics'])\n",
    "best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e00886",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_dir = Path('outputs/backtest')\n",
    "backtest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "curves = {\n",
    "    'equity': pd.Series(best_bt['equity_curve']),\n",
    "    'drawdown': pd.Series(best_bt['drawdown_curve']),\n",
    "    'exposure': pd.Series(best_bt['exposure_curve']),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "curves['equity'].plot(ax=axes[0], title='Equity curve (with fees/slippage)')\n",
    "curves['exposure'].plot(ax=axes[1], title='Position size (fraction of equity)')\n",
    "curves['drawdown'].plot(ax=axes[2], title='Drawdown')\n",
    "\n",
    "axes[1].set_ylabel('Sizing')\n",
    "axes[2].set_ylabel('Drawdown')\n",
    "axes[2].set_xlabel('Trades/time steps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = backtest_dir / '1h_best_equity.png'\n",
    "plt.savefig(plot_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"Saved risk-aware backtest plot to {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2fcec",
   "metadata": {},
   "source": [
    "## 13. Persist models and explainability artifacts\n",
    "Save LightGBM models, preprocessing objects, and SHAP ranking tables for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('outputs/models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for tf, res in results.items():\n",
    "    joblib.dump(res['model'], model_dir / f'{tf}_lgbm.pkl')\n",
    "    joblib.dump(res['preprocessing'], model_dir / f'{tf}_preprocess.pkl')\n",
    "\n",
    "if 'shap_df' in globals():\n",
    "    shap_df.to_csv(IMG_DIR / 'shap_importance_table.csv')\n",
    "    rank_df.to_csv(IMG_DIR / 'shap_rank_table.csv')\n",
    "    print('Saved SHAP tables under outputs/shap/')\n",
    "else:\n",
    "    print('SHAP tables not saved because shap_df is missing in this session.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59821548",
   "metadata": {},
   "source": [
    "\n",
    "## 14. Next Steps & Risk Layer\n",
    "- Apply strategy-specific filters (reversal/trend/volatility) using the `prob_up` outputs and indicators (RSI, bb_percent, macd_hist, EMA stacks, ATR).\n",
    "- Add risk management: ATR-based SL/TP, max risk per trade, exposure caps.\n",
    "- Extend to multi-asset or orderbook factors when data is available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
