{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6725d40",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End Indicator ML Strategy (BTCUSDT)\n",
    "\n",
    "Blueprint notebook to transform the provided feature-rich CSVs into a clean feature layer, label future returns, train a baseline ML model, and backtest simple probability-to-position mappings. The main focus is the 1H dataset, with comparison against 4H and 1D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86444ff1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Paths\n",
    "Load core dependencies and configure the file locations for each timeframe. The 1H file is treated as the primary source while keeping 4H and 1D available for quick comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"Locate project root that contains the data folder by walking upward.\"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        data_dir = candidate / \"data\"\n",
    "        if data_dir.exists():\n",
    "            return candidate\n",
    "    return cwd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_PATHS = {\n",
    "    \"1h\": PROJECT_ROOT / \"data\" / \"BINANCE_BTCUSDT.P, 60.csv\",\n",
    "    \"4h\": PROJECT_ROOT / \"data\" / \"BINANCE_BTCUSDT.P, 240.csv\",\n",
    "    \"1d\": PROJECT_ROOT / \"data\" / \"BINANCE_BTCUSDT.P, 1D.csv\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f7718",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Column Cleaning Rules\n",
    "Rename noisy columns into ML-friendly snake_case names, drop purely visual columns, and ensure duplicates are disambiguated. If you later discover the exact EMA periods, adjust `ema_fast`/`ema_slow` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ae688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RENAME_MAP = {\n",
    "    'time': 'time',\n",
    "    'open': 'open',\n",
    "    'high': 'high',\n",
    "    'low': 'low',\n",
    "    'close': 'close',\n",
    "    'Volume': 'volume',\n",
    "    'VWAP': 'vwap',\n",
    "    'Upper Band #1': 'bb1_upper',\n",
    "    'Lower Band #1': 'bb1_lower',\n",
    "    'Basis': 'bb_basis',\n",
    "    'Upper': 'bb_upper',\n",
    "    'Lower': 'bb_lower',\n",
    "    'Up Trend': 'trend_up',\n",
    "    'Down Trend': 'trend_down',\n",
    "    'EMA': 'ema_fast',\n",
    "    'EMA.1': 'ema_slow',\n",
    "    'Conversion Line': 'ichi_conv',\n",
    "    'Base Line': 'ichi_base',\n",
    "    'Lagging Span': 'ichi_lag',\n",
    "    'Leading Span A': 'ichi_span_a',\n",
    "    'Leading Span B': 'ichi_span_b',\n",
    "    'Upper.1': 'channel_upper',\n",
    "    'Average': 'channel_mid',\n",
    "    'Lower.1': 'channel_lower',\n",
    "    'RSI': 'rsi',\n",
    "    'RSI-based MA': 'rsi_ma',\n",
    "    'Regular Bullish': 'div_bull',\n",
    "    'Regular Bullish Label': 'div_bull_label',\n",
    "    'Regular Bearish': 'div_bear',\n",
    "    'Regular Bearish Label': 'div_bear_label',\n",
    "    'Histogram': 'macd_hist',\n",
    "    'MACD': 'macd',\n",
    "    'Signal': 'macd_signal',\n",
    "    'ATR': 'atr',\n",
    "    'K': 'stoch_k',\n",
    "    'D': 'stoch_d',\n",
    "    '%R': 'williams_r',\n",
    "}\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    'PlotCandle',\n",
    "]\n",
    "DROP_EXACT = {'Plot', 'Plot.1', 'Plot.2', 'div_bull_label', 'div_bear_label'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfad1a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Loading & Feature Layer Construction\n",
    "Helper functions to load a timeframe, clean columns, fill gaps, and add a few helper features (returns, Bollinger %B, Bollinger width). The result is the **feature layer** ready for labeling and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e318d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rename_and_filter_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    keep_cols = []\n",
    "    for col in df.columns:\n",
    "        if any(pattern in col for pattern in DROP_PATTERNS):\n",
    "            continue\n",
    "        if col in DROP_EXACT:\n",
    "            continue\n",
    "        keep_cols.append(col)\n",
    "    df = df[keep_cols]\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_feature_layer(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found: {path}. Resolved PROJECT_ROOT: {PROJECT_ROOT}.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = rename_and_filter_columns(df)\n",
    "\n",
    "    # Basic cleaning\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.sort_values(\"time\").drop_duplicates(subset=\"time\")\n",
    "    df = df.ffill().bfill()\n",
    "\n",
    "    # Derived helpers\n",
    "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
    "    df[\"bb_percent\"] = (df[\"close\"] - df[\"bb_lower\"]) / (df[\"bb_upper\"] - df[\"bb_lower\"])\n",
    "    df[\"bb_width\"] = (df[\"bb_upper\"] - df[\"bb_lower\"]) / df[\"bb_basis\"]\n",
    "\n",
    "    # Replace inf/nan from division\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539fb5e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Labeling (Binary Up/Down)\n",
    "Create a forward return label for a chosen horizon. By default we use 4 bars ahead. `y = 1` when the future return is positive, else `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6966425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_labels(df: pd.DataFrame, horizon: int = 4) -> pd.DataFrame:\n",
    "    future_price = df['close'].shift(-horizon)\n",
    "    df['future_return'] = future_price / df['close'] - 1\n",
    "    df['y'] = (df['future_return'] > 0).astype(int)\n",
    "    df = df.dropna(subset=['future_return'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba85abe",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Validation/Test Split (Time-Based)\n",
    "Use chronological splits (70/15/15). No shuffling is applied to respect temporal order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e668b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    X_train: pd.DataFrame\n",
    "    X_val: pd.DataFrame\n",
    "    X_test: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    y_val: pd.Series\n",
    "    y_test: pd.Series\n",
    "    future_val: pd.Series\n",
    "    future_test: pd.Series\n",
    "\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, feature_cols: List[str]) -> DatasetSplit:\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.7)\n",
    "    val_end = int(n * 0.85)\n",
    "\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end]\n",
    "    test = df.iloc[val_end:]\n",
    "\n",
    "    X_train, y_train = train[feature_cols], train['y']\n",
    "    X_val, y_val = val[feature_cols], val['y']\n",
    "    X_test, y_test = test[feature_cols], test['y']\n",
    "\n",
    "    return DatasetSplit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        future_val=val['future_return'],\n",
    "        future_test=test['future_return'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4b7f5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Modeling + Threshold Search\n",
    "Train a LightGBM classifier inside a scikit-learn pipeline (imputer + scaler). Validation Sharpe on the forward-return horizon is used to pick the best probability threshold for entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_sharpe(returns: np.ndarray, periods_per_year: int = 365) -> float:\n",
    "    ret = np.array(returns)\n",
    "    if ret.std() == 0:\n",
    "        return 0.0\n",
    "    return (ret.mean() * periods_per_year) / (ret.std() * math.sqrt(periods_per_year))\n",
    "\n",
    "\n",
    "def evaluate_threshold(probs: np.ndarray, future_returns: pd.Series, candidate_thr: List[float]):\n",
    "    best_sharpe, best_thr = -np.inf, None\n",
    "    for thr in candidate_thr:\n",
    "        positions = (probs > thr).astype(int)\n",
    "        strat_ret = positions * future_returns.values\n",
    "        sharpe = compute_sharpe(strat_ret)\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe, best_thr = sharpe, thr\n",
    "    return best_thr, best_sharpe\n",
    "\n",
    "\n",
    "def train_model(split: DatasetSplit, feature_cols: List[str]):\n",
    "    model = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\n",
    "            \"model\",\n",
    "            lgb.LGBMClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=-1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective=\"binary\",\n",
    "            ),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    model.fit(split.X_train, split.y_train)\n",
    "\n",
    "    val_probs = model.predict_proba(split.X_val)[:, 1]\n",
    "    candidate_thr = [0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "    best_thr, best_sharpe = evaluate_threshold(val_probs, split.future_val, candidate_thr)\n",
    "\n",
    "    test_probs = model.predict_proba(split.X_test)[:, 1]\n",
    "    positions_test = (test_probs > best_thr).astype(int)\n",
    "    strat_ret_test = positions_test * split.future_test.values\n",
    "\n",
    "    metrics = {\n",
    "        \"val_accuracy\": accuracy_score(split.y_val, (val_probs > 0.5).astype(int)),\n",
    "        \"test_accuracy\": accuracy_score(split.y_test, (test_probs > 0.5).astype(int)),\n",
    "        \"val_sharpe@best_thr\": best_sharpe,\n",
    "        \"test_sharpe@best_thr\": compute_sharpe(strat_ret_test),\n",
    "        \"threshold\": best_thr,\n",
    "    }\n",
    "\n",
    "    reports = {\n",
    "        \"val_report\": classification_report(split.y_val, (val_probs > 0.5).astype(int), digits=3),\n",
    "        \"test_report\": classification_report(split.y_test, (test_probs > 0.5).astype(int), digits=3),\n",
    "    }\n",
    "\n",
    "    return model, metrics, reports, val_probs, test_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941e999",
   "metadata": {},
   "source": [
    "\n",
    "## 7. End-to-End Runner (Per Timeframe)\n",
    "Combine all steps for a given timeframe and collect summary metrics. The primary focus is `1h`, with side-by-side stats for `4h` and `1d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79440a9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Data file not found: C:\\Users\\jefri\\backtest-indicator\\notebooks\\data\\BINANCE_BTCUSDT.P, 60.csv. Resolved PROJECT_ROOT: C:\\Users\\jefri\\backtest-indicator\\notebooks.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     14\u001b[0m     summary \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeframe\u001b[39m\u001b[38;5;124m\"\u001b[39m: key,\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetrics,\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: summary,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreports\u001b[39m\u001b[38;5;124m\"\u001b[39m: reports,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_probs,\n\u001b[0;32m     27\u001b[0m     }\n\u001b[1;32m---> 29\u001b[0m results \u001b[38;5;241m=\u001b[39m {tf: run_timeframe(tf) \u001b[38;5;28;01mfor\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m DATA_PATHS}\n\u001b[0;32m     30\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([results[tf][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m DATA_PATHS])\n\u001b[0;32m     31\u001b[0m summary_df\n",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m     summary \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeframe\u001b[39m\u001b[38;5;124m\"\u001b[39m: key,\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetrics,\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: summary,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreports\u001b[39m\u001b[38;5;124m\"\u001b[39m: reports,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_probs,\n\u001b[0;32m     27\u001b[0m     }\n\u001b[1;32m---> 29\u001b[0m results \u001b[38;5;241m=\u001b[39m {tf: \u001b[43mrun_timeframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m DATA_PATHS}\n\u001b[0;32m     30\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([results[tf][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m DATA_PATHS])\n\u001b[0;32m     31\u001b[0m summary_df\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mrun_timeframe\u001b[1;34m(key, horizon)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_timeframe\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, horizon: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_feature_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATHS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m add_labels(df, horizon\u001b[38;5;241m=\u001b[39mhorizon)\n\u001b[0;32m      5\u001b[0m     feature_cols \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m         col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture_return\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m col\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv_\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# drop visual divergence labels\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mbuild_feature_layer\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     15\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Resolved PROJECT_ROOT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[0;32m     22\u001b[0m df \u001b[38;5;241m=\u001b[39m rename_and_filter_columns(df)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Data file not found: C:\\Users\\jefri\\backtest-indicator\\notebooks\\data\\BINANCE_BTCUSDT.P, 60.csv. Resolved PROJECT_ROOT: C:\\Users\\jefri\\backtest-indicator\\notebooks."
     ]
    }
   ],
   "source": [
    "\n",
    "def run_timeframe(key: str, horizon: int = 4):\n",
    "    df = build_feature_layer(DATA_PATHS[key])\n",
    "    df = add_labels(df, horizon=horizon)\n",
    "\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in {'time', 'y', 'future_return'}\n",
    "        and not col.startswith('div_')  # drop visual divergence labels\n",
    "    ]\n",
    "\n",
    "    split = time_based_split(df, feature_cols)\n",
    "    model, metrics, reports, val_probs, test_probs = train_model(split, feature_cols)\n",
    "\n",
    "    summary = {\n",
    "        \"timeframe\": key,\n",
    "        \"rows\": len(df),\n",
    "        **metrics,\n",
    "    }\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"reports\": reports,\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"split\": split,\n",
    "        \"val_probs\": val_probs,\n",
    "        \"test_probs\": test_probs,\n",
    "    }\n",
    "\n",
    "results = {tf: run_timeframe(tf) for tf in DATA_PATHS}\n",
    "summary_df = pd.DataFrame([results[tf][\"summary\"] for tf in DATA_PATHS])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31c4b9",
   "metadata": {},
   "source": [
    "\n",
    "### Classification Reports (quick check)\n",
    "View precision/recall/F1 for each timeframe to understand base discrimination performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tf, res in results.items():\n",
    "    print(f\"=== {tf} ===\")\n",
    "    print(res['reports']['test_report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186adb",
   "metadata": {},
   "source": [
    "\n",
    "## 8. SHAP Explainability (Feature Importance)\n",
    "Use SHAP to see which indicators drive the `prob_up` predictions. Run on a subset for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment if shap is not available in your environment\n",
    "# %pip install shap\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "primary = results['1h']\n",
    "X_explain = primary['split'].X_test.sample(n=min(1000, len(primary['split'].X_test)), random_state=42)\n",
    "explainer = shap.TreeExplainer(primary['model'].named_steps['model'])\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_class1 = shap_values[1]\n",
    "else:\n",
    "    shap_values_class1 = shap_values\n",
    "\n",
    "shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'])\n",
    "shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'], plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59821548",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Next Steps & Risk Layer\n",
    "- Apply strategy-specific filters (reversal/trend/volatility) using the `prob_up` outputs and indicators (RSI, bb_percent, macd_hist, EMA stacks, ATR).\n",
    "- Add risk management: ATR-based SL/TP, max risk per trade, exposure caps.\n",
    "- Extend to multi-asset or orderbook factors when data is available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
