{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6725d40",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End Indicator ML Strategy (BTCUSDT)\n",
    "\n",
    "Blueprint notebook to transform the provided feature-rich CSVs into a clean feature layer, label future returns, train a baseline ML model, and backtest simple probability-to-position mappings. The main focus is the 1H dataset, with comparison against 4H and 1D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86444ff1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Paths\n",
    "Load core dependencies and configure the file locations for each timeframe. The 1H file is treated as the primary source while keeping 4H and 1D available for quick comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def discover_candidate_data_dirs() -> List[Path]:\n",
    "    \"\"\"Enumerate plausible data directories, including env overrides and repo root.\"\"\"\n",
    "    candidates: List[Path] = []\n",
    "    seen = set()\n",
    "\n",
    "    def add(path_like):\n",
    "        path = Path(path_like).resolve()\n",
    "        if path in seen:\n",
    "            return\n",
    "        seen.add(path)\n",
    "        candidates.append(path)\n",
    "\n",
    "    env_dir = os.getenv(\"DATA_DIR\")\n",
    "    if env_dir:\n",
    "        add(env_dir)\n",
    "\n",
    "    cwd = Path.cwd().resolve()\n",
    "    add(cwd / \"data\")\n",
    "    add(cwd / \"notebooks\" / \"data\")\n",
    "    for parent in cwd.parents:\n",
    "        add(parent / \"data\")\n",
    "        add(parent / \"notebooks\" / \"data\")\n",
    "\n",
    "    try:\n",
    "        repo_root = Path(\n",
    "            subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], text=True).strip()\n",
    "        )\n",
    "        add(repo_root / \"data\")\n",
    "        add(repo_root / \"notebooks\" / \"data\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return [p for p in candidates if p.exists()]\n",
    "\n",
    "\n",
    "def resolve_data_path(filename: str) -> Path:\n",
    "    data_dirs = discover_candidate_data_dirs()\n",
    "    attempts = []\n",
    "    for base in data_dirs:\n",
    "        candidate = base / filename\n",
    "        attempts.append(candidate)\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate data file '{filename}'. Checked: {attempts or ['<no data dirs found>']}. \"\n",
    "        \"Set DATA_DIR env var or place the file under one of the listed data directories.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_FILES = {\n",
    "    \"1h\": \"BINANCE_BTCUSDT.P, 60.csv\",\n",
    "    \"4h\": \"BINANCE_BTCUSDT.P, 240.csv\",\n",
    "    \"1d\": \"BINANCE_BTCUSDT.P, 1D.csv\",\n",
    "}\n",
    "DATA_PATHS = {tf: resolve_data_path(filename) for tf, filename in DATA_FILES.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_layer(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data file not found: {path}. Checked candidate data dirs: {discover_candidate_data_dirs()}.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = rename_and_filter_columns(df)\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.sort_values(\"time\").drop_duplicates(subset=[\"time\"])\n",
    "    df = df.set_index(\"time\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.ffill().bfill()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
    "    df[\"bb_percent\"] = (df[\"close\"] - df[\"bb_lower\"]) / (df[\"bb_upper\"] - df[\"bb_lower\"])\n",
    "    df[\"bb_width\"] = (df[\"bb_upper\"] - df[\"bb_lower\"]) / df[\"bb_basis\"]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RENAME_MAP = {\n",
    "    'time': 'time',\n",
    "    'open': 'open',\n",
    "    'high': 'high',\n",
    "    'low': 'low',\n",
    "    'close': 'close',\n",
    "    'Volume': 'volume',\n",
    "    'VWAP': 'vwap',\n",
    "    'Upper Band #1': 'bb1_upper',\n",
    "    'Lower Band #1': 'bb1_lower',\n",
    "    'Basis': 'bb_basis',\n",
    "    'Upper': 'bb_upper',\n",
    "    'Lower': 'bb_lower',\n",
    "    'Up Trend': 'trend_up',\n",
    "    'Down Trend': 'trend_down',\n",
    "    'EMA': 'ema_fast',\n",
    "    'EMA.1': 'ema_slow',\n",
    "    'Conversion Line': 'ichi_conv',\n",
    "    'Base Line': 'ichi_base',\n",
    "    'Lagging Span': 'ichi_lag',\n",
    "    'Leading Span A': 'ichi_span_a',\n",
    "    'Leading Span B': 'ichi_span_b',\n",
    "    'Upper.1': 'channel_upper',\n",
    "    'Average': 'channel_mid',\n",
    "    'Lower.1': 'channel_lower',\n",
    "    'RSI': 'rsi',\n",
    "    'RSI-based MA': 'rsi_ma',\n",
    "    'Regular Bullish': 'div_bull',\n",
    "    'Regular Bullish Label': 'div_bull_label',\n",
    "    'Regular Bearish': 'div_bear',\n",
    "    'Regular Bearish Label': 'div_bear_label',\n",
    "    'Histogram': 'macd_hist',\n",
    "    'MACD': 'macd',\n",
    "    'Signal': 'macd_signal',\n",
    "    'ATR': 'atr',\n",
    "    'K': 'stoch_k',\n",
    "    'D': 'stoch_d',\n",
    "    '%R': 'williams_r',\n",
    "}\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    'PlotCandle',\n",
    "]\n",
    "DROP_EXACT = {'Plot', 'Plot.1', 'Plot.2', 'div_bull_label', 'div_bear_label'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfad1a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Loading & Feature Layer Construction\n",
    "Helper functions to load a timeframe, clean columns, fill gaps, and add a few helper features (returns, Bollinger %B, Bollinger width). The result is the **feature layer** ready for labeling and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e318d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_filter_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    keep_cols = []\n",
    "    for col in df.columns:\n",
    "        if any(pattern in col for pattern in DROP_PATTERNS):\n",
    "            continue\n",
    "        if col in DROP_EXACT:\n",
    "            continue\n",
    "        keep_cols.append(col)\n",
    "    return df[keep_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539fb5e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Labeling (Binary Up/Down)\n",
    "Create a forward return label for a chosen horizon. By default we use 4 bars ahead. `y = 1` when the future return is positive, else `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_labels(df: pd.DataFrame, horizon: int = 4) -> pd.DataFrame:\n",
    "    future_price = df['close'].shift(-horizon)\n",
    "    df['future_return'] = future_price / df['close'] - 1\n",
    "    df['y'] = (df['future_return'] > 0).astype(int)\n",
    "    df = df.dropna(subset=['future_return'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba85abe",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Validation/Test Split (Time-Based)\n",
    "Use chronological splits (70/15/15). No shuffling is applied to respect temporal order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e668b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    X_train: pd.DataFrame\n",
    "    X_val: pd.DataFrame\n",
    "    X_test: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    y_val: pd.Series\n",
    "    y_test: pd.Series\n",
    "    future_val: pd.Series\n",
    "    future_test: pd.Series\n",
    "\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, feature_cols: List[str]) -> DatasetSplit:\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.7)\n",
    "    val_end = int(n * 0.85)\n",
    "\n",
    "    train = df.iloc[:train_end]\n",
    "    val = df.iloc[train_end:val_end]\n",
    "    test = df.iloc[val_end:]\n",
    "\n",
    "    X_train, y_train = train[feature_cols], train['y']\n",
    "    X_val, y_val = val[feature_cols], val['y']\n",
    "    X_test, y_test = test[feature_cols], test['y']\n",
    "\n",
    "    return DatasetSplit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        future_val=val['future_return'],\n",
    "        future_test=test['future_return'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4b7f5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Modeling + Threshold Search\n",
    "Train a LightGBM classifier inside a scikit-learn pipeline (imputer + scaler). Validation Sharpe on the forward-return horizon is used to pick the best probability threshold for entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sharpe(returns: np.ndarray, periods_per_year: int = 365) -> float:\n",
    "    ret = np.array(returns)\n",
    "    if ret.std() == 0:\n",
    "        return 0.0\n",
    "    return (ret.mean() * periods_per_year) / (ret.std() * math.sqrt(periods_per_year))\n",
    "\n",
    "\n",
    "def evaluate_threshold(probs: np.ndarray, future_returns: pd.Series, candidate_thr: List[float]):\n",
    "    best_sharpe, best_thr = -np.inf, None\n",
    "    for thr in candidate_thr:\n",
    "        positions = (probs > thr).astype(int)\n",
    "        strat_ret = positions * future_returns.values\n",
    "        sharpe = compute_sharpe(strat_ret)\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe, best_thr = sharpe, thr\n",
    "    return best_thr, best_sharpe\n",
    "\n",
    "\n",
    "def train_model(split: DatasetSplit, feature_cols: List[str]):\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    imputer.fit(split.X_train)\n",
    "    scaler.fit(imputer.transform(split.X_train))\n",
    "\n",
    "    def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        arr = imputer.transform(df)\n",
    "        arr = scaler.transform(arr)\n",
    "        return pd.DataFrame(arr, columns=feature_cols, index=df.index)\n",
    "\n",
    "    X_train = transform(split.X_train)\n",
    "    X_val = transform(split.X_val)\n",
    "    X_test = transform(split.X_test)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"binary\",\n",
    "    )\n",
    "    model.fit(X_train, split.y_train)\n",
    "\n",
    "    val_probs = model.predict_proba(X_val)[:, 1]\n",
    "    candidate_thr = [0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "    best_thr, best_sharpe = evaluate_threshold(val_probs, split.future_val, candidate_thr)\n",
    "\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    positions_test = (test_probs > best_thr).astype(int)\n",
    "    strat_ret_test = positions_test * split.future_test.values\n",
    "\n",
    "    metrics = {\n",
    "        \"val_accuracy\": accuracy_score(split.y_val, (val_probs > 0.5).astype(int)),\n",
    "        \"test_accuracy\": accuracy_score(split.y_test, (test_probs > 0.5).astype(int)),\n",
    "        \"val_sharpe@best_thr\": best_sharpe,\n",
    "        \"test_sharpe@best_thr\": compute_sharpe(strat_ret_test),\n",
    "        \"threshold\": best_thr,\n",
    "    }\n",
    "\n",
    "    reports = {\n",
    "        \"val_report\": classification_report(split.y_val, (val_probs > 0.5).astype(int), digits=3),\n",
    "        \"test_report\": classification_report(split.y_test, (test_probs > 0.5).astype(int), digits=3),\n",
    "    }\n",
    "\n",
    "    processed = {\"train\": X_train, \"val\": X_val, \"test\": X_test}\n",
    "\n",
    "    return model, metrics, reports, val_probs, test_probs, processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941e999",
   "metadata": {},
   "source": [
    "\n",
    "## 7. End-to-End Runner (Per Timeframe)\n",
    "Combine all steps for a given timeframe and collect summary metrics. The primary focus is `1h`, with side-by-side stats for `4h` and `1d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79440a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_timeframe(key: str, horizon: int = 4):\n",
    "    df = build_feature_layer(DATA_PATHS[key])\n",
    "    df = add_labels(df, horizon=horizon)\n",
    "\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in {'time', 'y', 'future_return'}\n",
    "        and not col.startswith('div_')  # drop visual divergence labels\n",
    "    ]\n",
    "\n",
    "    split = time_based_split(df, feature_cols)\n",
    "    model, metrics, reports, val_probs, test_probs, processed = train_model(split, feature_cols)\n",
    "\n",
    "    summary = {\n",
    "        \"timeframe\": key,\n",
    "        \"rows\": len(df),\n",
    "        **metrics,\n",
    "    }\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"reports\": reports,\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"split\": split,\n",
    "        \"processed_features\": processed,\n",
    "        \"val_probs\": val_probs,\n",
    "        \"test_probs\": test_probs,\n",
    "    }\n",
    "\n",
    "results = {tf: run_timeframe(tf) for tf in DATA_PATHS}\n",
    "summary_df = pd.DataFrame([results[tf][\"summary\"] for tf in DATA_PATHS])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31c4b9",
   "metadata": {},
   "source": [
    "\n",
    "### Classification Reports (quick check)\n",
    "View precision/recall/F1 for each timeframe to understand base discrimination performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tf, res in results.items():\n",
    "    print(f\"=== {tf} ===\")\n",
    "    print(res['reports']['test_report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76186adb",
   "metadata": {},
   "source": [
    "\n",
    "## 8. SHAP Explainability (Feature Importance)\n",
    "Use SHAP to see which indicators drive the `prob_up` predictions. Run on a subset for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if shap is not available in your environment\n",
    "# %pip install shap\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except ModuleNotFoundError:\n",
    "    print(\"shap is not installed in this environment. Install with `pip install shap` to view explainability plots.\")\n",
    "else:\n",
    "    shap.initjs()\n",
    "\n",
    "    primary = results['1h']\n",
    "    X_explain = primary['processed_features']['test'].sample(\n",
    "        n=min(1000, len(primary['processed_features']['test'])), random_state=42\n",
    "    )\n",
    "    explainer = shap.TreeExplainer(primary['model'])\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values_class1 = shap_values[1]\n",
    "    else:\n",
    "        shap_values_class1 = shap_values\n",
    "\n",
    "    shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'])\n",
    "    shap.summary_plot(shap_values_class1, X_explain, feature_names=primary['feature_cols'], plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59821548",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Next Steps & Risk Layer\n",
    "- Apply strategy-specific filters (reversal/trend/volatility) using the `prob_up` outputs and indicators (RSI, bb_percent, macd_hist, EMA stacks, ATR).\n",
    "- Add risk management: ATR-based SL/TP, max risk per trade, exposure caps.\n",
    "- Extend to multi-asset or orderbook factors when data is available.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
