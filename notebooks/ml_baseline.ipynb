{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439a5757",
   "metadata": {},
   "source": [
    "# ML Baseline Models\n",
    "Notebook ini membangun baseline machine learning berbasis fitur OHLCV harian ETHUSDT.\n",
    "Dataset processed akan disimpan ulang agar dapat dipakai ulang oleh pipeline berikutnya,\n",
    "kemudian model linear (Lasso/ElasticNet) serta LightGBM dievaluasi menggunakan skema TimeSeriesSplit\n",
    "dan window out-of-sample tahun 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0e480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def locate_project_root() -> Path:\n",
    "    \"\"\"Cari folder proyek yang menyimpan data dan notebook.\"\"\"\n",
    "\n",
    "    current = Path.cwd().resolve()\n",
    "    for candidate in (current, *current.parents):\n",
    "        if (candidate / 'data').exists() and (candidate / 'notebooks').exists():\n",
    "            return candidate\n",
    "    return current\n",
    "\n",
    "\n",
    "PROJECT_ROOT = locate_project_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'OKX_ETHUSDT.P, 60.csv'\n",
    "PROCESSED_PATH = PROJECT_ROOT / 'data' / 'processed' / 'ethusdt_hourly_features.csv'\n",
    "MODEL_DIR = PROJECT_ROOT / 'outputs' / 'models'\n",
    "PREDICTION_DIR = PROJECT_ROOT / 'outputs' / 'predictions'\n",
    "from src.performance.metrics import DEFAULT_BARS_PER_YEAR, summarise_fold_performance\n",
    "BARS_PER_DAY = 24.0\n",
    "BARS_PER_YEAR = float(BARS_PER_DAY * 365.0)\n",
    "HORIZON_HOURS = 5\n",
    "RETURN_TYPE = 'simple'\n",
    "WALKFORWARD_SPLITS = 5\n",
    "\n",
    "PROCESSED_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PREDICTION_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e9a356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset memiliki 4559 baris dengan 11 fitur.\n",
      "Dataset tersimpan ke C:\\Users\\jefri\\backtest\\data\\processed\\ethusdt_hourly_features.csv\n"
     ]
    }
   ],
   "source": [
    "def load_ohlcv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load OHLCV CSV dan bersihkan kolomnya.\"\"\"\n",
    "\n",
    "    path = Path(path).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset {path} tidak tersedia. Download data terlebih dahulu atau ubah DATA_PATH.\"\n",
    "        )\n",
    "\n",
    "    raw = pd.read_csv(path)\n",
    "    raw.columns = [col.strip().lower().replace(' ', '_') for col in raw.columns]\n",
    "    keep = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    missing = [col for col in keep if col not in raw.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns {missing} in {path}\")\n",
    "    ohlcv = raw[keep].copy()\n",
    "    ohlcv['time'] = pd.to_datetime(ohlcv['time'], utc=True)\n",
    "    numeric_cols = [col for col in keep if col != 'time']\n",
    "    ohlcv[numeric_cols] = ohlcv[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    ohlcv = ohlcv.dropna().set_index('time').sort_index()\n",
    "    return ohlcv\n",
    "\n",
    "\n",
    "def engineer_features(ohlcv: pd.DataFrame) -> pd.DataFrame:\n",
    "    close = ohlcv['close'].astype(float)\n",
    "    volume = ohlcv['volume'].astype(float)\n",
    "    features = pd.DataFrame(index=ohlcv.index)\n",
    "    features['ret_1d'] = close.pct_change(1)\n",
    "    features['ret_5d'] = close.pct_change(5)\n",
    "    features['ret_20d'] = close.pct_change(20)\n",
    "    features['momentum_20d'] = features['ret_1d'].rolling(20).mean()\n",
    "    features['momentum_60d'] = features['ret_1d'].rolling(60).mean()\n",
    "    features['volatility_20d'] = features['ret_1d'].rolling(20).std()\n",
    "    features['volatility_60d'] = features['ret_1d'].rolling(60).std()\n",
    "    features['volume_change'] = volume.pct_change(1)\n",
    "    features['volume_zscore_20d'] = (volume - volume.rolling(20).mean()) / volume.rolling(20).std()\n",
    "    ema_fast = close.ewm(span=20, adjust=False).mean()\n",
    "    ema_slow = close.ewm(span=60, adjust=False).mean()\n",
    "    features['price_ema_spread'] = (ema_fast - ema_slow) / ema_slow\n",
    "    features['high_low_range'] = (ohlcv['high'] - ohlcv['low']) / close\n",
    "    return features\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    ohlcv: pd.DataFrame,\n",
    "    horizon: int = HORIZON_HOURS,\n",
    "    return_type: str = RETURN_TYPE,\n",
    ") -> pd.DataFrame:\n",
    "    features = engineer_features(ohlcv)\n",
    "    if return_type == 'log':\n",
    "        forward_returns = (np.log(ohlcv['close'].shift(-horizon)) - np.log(ohlcv['close']))\n",
    "    elif return_type == 'simple':\n",
    "        forward_returns = ohlcv['close'].pct_change(horizon).shift(-horizon)\n",
    "    else:\n",
    "        raise ValueError(\"return_type harus 'simple' atau 'log'.\")\n",
    "    labels = (forward_returns > 0).astype(int).rename('target')\n",
    "    dataset = features.join(labels).join(forward_returns.rename('future_return'))\n",
    "    dataset = dataset.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "ohlcv = load_ohlcv(DATA_PATH)\n",
    "dataset = build_dataset(ohlcv, horizon=HORIZON_HOURS, return_type=RETURN_TYPE)\n",
    "print(f\"Dataset memiliki {len(dataset)} baris dengan {dataset.shape[1]-2} fitur.\")\n",
    "dataset.to_csv(PROCESSED_PATH)\n",
    "print(f\"Dataset tersimpan ke {PROCESSED_PATH}\")\n",
    "\n",
    "dataset_metadata = pd.DataFrame(\n",
    "    [\n",
    "        (\"horizon_hours\", HORIZON_HOURS),\n",
    "        (\"return_type\", RETURN_TYPE),\n",
    "        (\"target_definition\", \"1 jika forward_return > 0 else 0\"),\n",
    "        (\"rows\", len(dataset)),\n",
    "        (\"start_time\", dataset.index.min()),\n",
    "        (\"end_time\", dataset.index.max()),\n",
    "    ],\n",
    "    columns=[\"key\", \"value\"],\n",
    ").set_index(\"key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef6c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performa walk-forward (train folds):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>n_bars</th>\n",
       "      <th>mean_return</th>\n",
       "      <th>volatility</th>\n",
       "      <th>annualised_vol</th>\n",
       "      <th>sharpe_ratio</th>\n",
       "      <th>hit_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-07 23:00:00+00:00</td>\n",
       "      <td>2025-06-08 18:00:00+00:00</td>\n",
       "      <td>764.0</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.018246</td>\n",
       "      <td>1.707748</td>\n",
       "      <td>11.465297</td>\n",
       "      <td>0.544503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-07 23:00:00+00:00</td>\n",
       "      <td>2025-07-10 09:00:00+00:00</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>1.564824</td>\n",
       "      <td>8.372955</td>\n",
       "      <td>0.525279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-07 23:00:00+00:00</td>\n",
       "      <td>2025-08-11 00:00:00+00:00</td>\n",
       "      <td>2282.0</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.016035</td>\n",
       "      <td>1.500774</td>\n",
       "      <td>11.644270</td>\n",
       "      <td>0.543821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-07 23:00:00+00:00</td>\n",
       "      <td>2025-09-11 15:00:00+00:00</td>\n",
       "      <td>3041.0</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>1.516551</td>\n",
       "      <td>9.087557</td>\n",
       "      <td>0.537981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-07 23:00:00+00:00</td>\n",
       "      <td>2025-10-13 06:00:00+00:00</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>1.484669</td>\n",
       "      <td>7.054783</td>\n",
       "      <td>0.528158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start_time                  end_time  n_bars  mean_return  \\\n",
       "fold                                                                            \n",
       "0    2025-05-07 23:00:00+00:00 2025-06-08 18:00:00+00:00   764.0     0.002235   \n",
       "1    2025-05-07 23:00:00+00:00 2025-07-10 09:00:00+00:00  1523.0     0.001496   \n",
       "2    2025-05-07 23:00:00+00:00 2025-08-11 00:00:00+00:00  2282.0     0.001995   \n",
       "3    2025-05-07 23:00:00+00:00 2025-09-11 15:00:00+00:00  3041.0     0.001573   \n",
       "4    2025-05-07 23:00:00+00:00 2025-10-13 06:00:00+00:00  3800.0     0.001196   \n",
       "\n",
       "      volatility  annualised_vol  sharpe_ratio  hit_rate  \n",
       "fold                                                      \n",
       "0       0.018246        1.707748     11.465297  0.544503  \n",
       "1       0.016719        1.564824      8.372955  0.525279  \n",
       "2       0.016035        1.500774     11.644270  0.543821  \n",
       "3       0.016203        1.516551      9.087557  0.537981  \n",
       "4       0.015863        1.484669      7.054783  0.528158  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performa walk-forward (test folds):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>n_bars</th>\n",
       "      <th>mean_return</th>\n",
       "      <th>volatility</th>\n",
       "      <th>annualised_vol</th>\n",
       "      <th>sharpe_ratio</th>\n",
       "      <th>hit_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-08 19:00:00+00:00</td>\n",
       "      <td>2025-07-10 09:00:00+00:00</td>\n",
       "      <td>759.0</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>1.402921</td>\n",
       "      <td>4.691560</td>\n",
       "      <td>0.505929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-10 10:00:00+00:00</td>\n",
       "      <td>2025-08-11 00:00:00+00:00</td>\n",
       "      <td>759.0</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.014513</td>\n",
       "      <td>1.358366</td>\n",
       "      <td>19.325181</td>\n",
       "      <td>0.581028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-11 01:00:00+00:00</td>\n",
       "      <td>2025-09-11 15:00:00+00:00</td>\n",
       "      <td>759.0</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.016636</td>\n",
       "      <td>1.557013</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>0.520422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-11 16:00:00+00:00</td>\n",
       "      <td>2025-10-13 06:00:00+00:00</td>\n",
       "      <td>759.0</td>\n",
       "      <td>-0.000317</td>\n",
       "      <td>0.014318</td>\n",
       "      <td>1.340084</td>\n",
       "      <td>-2.073497</td>\n",
       "      <td>0.488801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-13 07:00:00+00:00</td>\n",
       "      <td>2025-11-13 21:00:00+00:00</td>\n",
       "      <td>759.0</td>\n",
       "      <td>-0.001500</td>\n",
       "      <td>0.018209</td>\n",
       "      <td>1.704280</td>\n",
       "      <td>-7.709342</td>\n",
       "      <td>0.492754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    start_time                  end_time  n_bars  mean_return  \\\n",
       "fold                                                                            \n",
       "0    2025-06-08 19:00:00+00:00 2025-07-10 09:00:00+00:00   759.0     0.000751   \n",
       "1    2025-07-10 10:00:00+00:00 2025-08-11 00:00:00+00:00   759.0     0.002997   \n",
       "2    2025-08-11 01:00:00+00:00 2025-09-11 15:00:00+00:00   759.0     0.000306   \n",
       "3    2025-09-11 16:00:00+00:00 2025-10-13 06:00:00+00:00   759.0    -0.000317   \n",
       "4    2025-10-13 07:00:00+00:00 2025-11-13 21:00:00+00:00   759.0    -0.001500   \n",
       "\n",
       "      volatility  annualised_vol  sharpe_ratio  hit_rate  \n",
       "fold                                                      \n",
       "0       0.014989        1.402921      4.691560  0.505929  \n",
       "1       0.014513        1.358366     19.325181  0.581028  \n",
       "2       0.016636        1.557013      1.718917  0.520422  \n",
       "3       0.014318        1.340084     -2.073497  0.488801  \n",
       "4       0.018209        1.704280     -7.709342  0.492754  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_columns = [col for col in dataset.columns if col not in [\"target\", \"future_return\"]]\n",
    "\n",
    "splitter = TimeSeriesSplit(n_splits=WALKFORWARD_SPLITS)\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "split_records = []\n",
    "\n",
    "for fold_id, (train_idx, test_idx) in enumerate(splitter.split(dataset)):\n",
    "    fold_train = dataset.iloc[train_idx].copy()\n",
    "    fold_test = dataset.iloc[test_idx].copy()\n",
    "    train_folds.append((fold_id, fold_train))\n",
    "    test_folds.append((fold_id, fold_test))\n",
    "    split_records.append(\n",
    "        {\n",
    "            \"fold\": fold_id,\n",
    "            \"train_start_time\": fold_train.index.min(),\n",
    "            \"train_end_time\": fold_train.index.max(),\n",
    "            \"test_start_time\": fold_test.index.min(),\n",
    "            \"test_end_time\": fold_test.index.max(),\n",
    "            \"n_train\": len(fold_train),\n",
    "            \"n_test\": len(fold_test),\n",
    "        }\n",
    "    )\n",
    "\n",
    "if not train_folds or not test_folds:\n",
    "    raise ValueError(\"TimeSeriesSplit gagal menghasilkan fold untuk dataset.\")\n",
    "\n",
    "train = train_folds[-1][1]\n",
    "test = test_folds[-1][1]\n",
    "\n",
    "train_split = pd.concat({fold: frame for fold, frame in train_folds}, names=[\"fold\", \"time\"])\n",
    "test_split = pd.concat({fold: frame for fold, frame in test_folds}, names=[\"fold\", \"time\"])\n",
    "cv_split_summary = pd.DataFrame(split_records).set_index(\"fold\")\n",
    "\n",
    "train_fold_performance = summarise_fold_performance(\n",
    "    train_folds, return_column='future_return', bars_per_year=BARS_PER_YEAR\n",
    ")\n",
    "test_fold_performance = summarise_fold_performance(\n",
    "    test_folds, return_column='future_return', bars_per_year=BARS_PER_YEAR\n",
    ")\n",
    "print(\"Performa walk-forward (train folds):\")\n",
    "display(train_fold_performance)\n",
    "print(\"Performa walk-forward (test folds):\")\n",
    "display(test_fold_performance)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(\n",
    "    scaler.fit_transform(train[feature_columns]),\n",
    "    index=train.index,\n",
    "    columns=feature_columns,\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    scaler.transform(test[feature_columns]),\n",
    "    index=test.index,\n",
    "    columns=feature_columns,\n",
    ")\n",
    "\n",
    "def drop_low_variance_features(\n",
    "    X_tr: pd.DataFrame, X_te: pd.DataFrame, tol: float = 1e-9\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, list[str]]:\n",
    "    variances = X_tr.var(axis=0)\n",
    "    keep_cols = variances[variances > tol].index.tolist()\n",
    "    dropped = sorted(set(X_tr.columns) - set(keep_cols))\n",
    "    if dropped:\n",
    "        print(\n",
    "            \"Menghapus fitur dengan varians sangat kecil: \" + \", \".join(dropped)\n",
    "        )\n",
    "    return X_tr[keep_cols], X_te[keep_cols], keep_cols\n",
    "\n",
    "X_train, X_test, feature_columns = drop_low_variance_features(X_train, X_test)\n",
    "\n",
    "y_train = train[\"target\"]\n",
    "y_test = test[\"target\"]\n",
    "y_test_returns = test[\"future_return\"]\n",
    "\n",
    "cv_splits = min(WALKFORWARD_SPLITS, len(train) - 1)\n",
    "if cv_splits < 2:\n",
    "    raise ValueError(\"Dataset train terlalu pendek untuk membuat CV splits.\")\n",
    "tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "\n",
    "def get_probabilities(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        decision = model.decision_function(X)\n",
    "        return 1.0 / (1.0 + np.exp(-decision))\n",
    "    preds = model.predict(X)\n",
    "    return preds.astype(float)\n",
    "\n",
    "def sharpe_ratio(signal: pd.Series, realized_returns: pd.Series, periods: int = 252) -> float:\n",
    "    pnl = signal * realized_returns\n",
    "    std = pnl.std(ddof=0)\n",
    "    if std == 0 or np.isnan(std):\n",
    "        return 0.0\n",
    "    return pnl.mean() / std * np.sqrt(periods)\n",
    "\n",
    "def rolling_cv_metrics(model, X: pd.DataFrame, y: pd.Series, splitter: TimeSeriesSplit):\n",
    "    preds = pd.Series(index=y.index, dtype=float)\n",
    "    accs, aucs = [], []\n",
    "    for train_idx, val_idx in splitter.split(X):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        estimator = clone(model)\n",
    "        estimator.fit(X_tr, y_tr)\n",
    "        probs = get_probabilities(estimator, X_val)\n",
    "        preds.iloc[val_idx] = probs\n",
    "        accs.append(accuracy_score(y_val, (probs >= 0.5).astype(int)))\n",
    "        try:\n",
    "            aucs.append(roc_auc_score(y_val, probs))\n",
    "        except ValueError:\n",
    "            aucs.append(np.nan)\n",
    "    return preds, {\"cv_accuracy\": float(np.nanmean(accs)), \"cv_auc\": float(np.nanmean(aucs))}\n",
    "\n",
    "def fit_and_evaluate(model, X_tr, y_tr, X_te, y_te, realized_returns):\n",
    "    estimator = clone(model)\n",
    "    estimator.fit(X_tr, y_tr)\n",
    "    probs = get_probabilities(estimator, X_te)\n",
    "    predictions = (probs >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_te, predictions)\n",
    "    auc = roc_auc_score(y_te, probs)\n",
    "    signal = pd.Series(probs, index=y_te.index)\n",
    "    signal = 2 * signal - 1\n",
    "    sharpe = sharpe_ratio(signal, realized_returns.loc[signal.index], periods=24 * 365)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"roc_auc\": float(auc),\n",
    "        \"signal_sharpe\": float(sharpe),\n",
    "    }\n",
    "    return estimator, probs, signal, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd891f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lasso] CV metrics: {'cv_accuracy': 0.5071090047393365, 'cv_auc': 0.5024398434382649}\n",
      "[lasso] Test metrics: {'accuracy': 0.5046113306982872, 'roc_auc': 0.5204805889297868, 'signal_sharpe': -4.529764479797216}\n",
      "[elasticnet] CV metrics: {'cv_accuracy': 0.5077409162717219, 'cv_auc': 0.502076034847968}\n",
      "[elasticnet] Test metrics: {'accuracy': 0.5019762845849802, 'roc_auc': 0.520876449753455, 'signal_sharpe': -4.540199898666112}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_accuracy</th>\n",
       "      <th>cv_auc</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_signal_sharpe</th>\n",
       "      <th>deployment_decision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lasso</th>\n",
       "      <td>0.507109</td>\n",
       "      <td>0.502440</td>\n",
       "      <td>0.504611</td>\n",
       "      <td>0.520481</td>\n",
       "      <td>-4.529764</td>\n",
       "      <td>reject_negative_sharpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elasticnet</th>\n",
       "      <td>0.507741</td>\n",
       "      <td>0.502076</td>\n",
       "      <td>0.501976</td>\n",
       "      <td>0.520876</td>\n",
       "      <td>-4.540200</td>\n",
       "      <td>reject_negative_sharpe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cv_accuracy    cv_auc  test_accuracy  test_roc_auc  \\\n",
       "model                                                            \n",
       "lasso          0.507109  0.502440       0.504611      0.520481   \n",
       "elasticnet     0.507741  0.502076       0.501976      0.520876   \n",
       "\n",
       "            test_signal_sharpe     deployment_decision  \n",
       "model                                                   \n",
       "lasso                -4.529764  reject_negative_sharpe  \n",
       "elasticnet           -4.540200  reject_negative_sharpe  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_models = {\n",
    "    \"lasso\": LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=5000, random_state=42),\n",
    "    \"elasticnet\": LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        solver=\"saga\",\n",
    "        l1_ratio=0.5,\n",
    "        max_iter=5000,\n",
    "        random_state=42,\n",
    "    ),\n",
    "}\n",
    "\n",
    "linear_results = {}\n",
    "for name, model in linear_models.items():\n",
    "    cv_preds, cv_metrics = rolling_cv_metrics(model, X_train, y_train, tscv)\n",
    "    estimator, probs, signal, test_metrics = fit_and_evaluate(\n",
    "        model, X_train, y_train, X_test, y_test, y_test_returns\n",
    "    )\n",
    "    linear_results[name] = {\n",
    "        \"model\": estimator,\n",
    "        \"cv_metrics\": cv_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "    }\n",
    "    print(f\"[{name}] CV metrics: {cv_metrics}\")\n",
    "    print(f\"[{name}] Test metrics: {test_metrics}\")\n",
    "linear_results\n",
    "\n",
    "linear_metrics = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        name: {\n",
    "            \"cv_accuracy\": result['cv_metrics'].get('cv_accuracy'),\n",
    "            \"cv_auc\": result['cv_metrics'].get('cv_auc'),\n",
    "            \"test_accuracy\": result['test_metrics'].get('accuracy'),\n",
    "            \"test_roc_auc\": result['test_metrics'].get('roc_auc'),\n",
    "            \"test_signal_sharpe\": result['test_metrics'].get('signal_sharpe'),\n",
    "        }\n",
    "        for name, result in linear_results.items()\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").rename_axis(\"model\")\n",
    "linear_metrics[\"deployment_decision\"] = np.where(\n",
    "    linear_metrics[\"test_signal_sharpe\"] > 0,\n",
    "    \"candidate\",\n",
    "    \"reject_negative_sharpe\",\n",
    ")\n",
    "linear_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5b0ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 347, number of negative: 288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2332\n",
      "[LightGBM] [Info] Number of data points in the train set: 635, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546457 -> initscore=0.186364\n",
      "[LightGBM] [Info] Start training from score 0.186364\n",
      "[LightGBM] [Info] Number of positive: 673, number of negative: 595\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1268, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.530757 -> initscore=0.123184\n",
      "[LightGBM] [Info] Start training from score 0.123184\n",
      "[LightGBM] [Info] Number of positive: 1025, number of negative: 876\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 1901, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.539190 -> initscore=0.157082\n",
      "[LightGBM] [Info] Start training from score 0.157082\n",
      "[LightGBM] [Info] Number of positive: 1373, number of negative: 1161\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 2534, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.541831 -> initscore=0.167716\n",
      "[LightGBM] [Info] Start training from score 0.167716\n",
      "[LightGBM] [Info] Number of positive: 1705, number of negative: 1462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 3167, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.538364 -> initscore=0.153760\n",
      "[LightGBM] [Info] Start training from score 0.153760\n",
      "[LightGBM] [Info] Number of positive: 2007, number of negative: 1793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2805\n",
      "[LightGBM] [Info] Number of data points in the train set: 3800, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.528158 -> initscore=0.112751\n",
      "[LightGBM] [Info] Start training from score 0.112751\n",
      "[lightgbm] CV metrics: {'cv_accuracy': 0.509952606635071, 'cv_auc': 0.5240223663895429}\n",
      "[lightgbm] Test metrics: {'accuracy': 0.541501976284585, 'roc_auc': 0.5476630321550109, 'signal_sharpe': 5.469825503092349}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_accuracy</th>\n",
       "      <th>cv_auc</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_signal_sharpe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>0.509953</td>\n",
       "      <td>0.524022</td>\n",
       "      <td>0.541502</td>\n",
       "      <td>0.547663</td>\n",
       "      <td>5.469826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_accuracy    cv_auc  test_accuracy  test_roc_auc  \\\n",
       "model                                                          \n",
       "lightgbm     0.509953  0.524022       0.541502      0.547663   \n",
       "\n",
       "          test_signal_sharpe  \n",
       "model                         \n",
       "lightgbm            5.469826  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lgb = X_train.astype(np.float32)\n",
    "X_test_lgb = X_test.astype(np.float32)\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_samples=5,\n",
    "    min_child_weight=1e-3,\n",
    "    feature_pre_filter=False,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "_, lgb_cv_metrics = rolling_cv_metrics(lgb_model, X_train_lgb, y_train, tscv)\n",
    "lgb_fitted, lgb_probs, lgb_signal, lgb_test_metrics = fit_and_evaluate(\n",
    "    lgb_model, X_train_lgb, y_train, X_test_lgb, y_test, y_test_returns\n",
    ")\n",
    "\n",
    "print(\"[lightgbm] CV metrics:\", lgb_cv_metrics)\n",
    "print(\"[lightgbm] Test metrics:\", lgb_test_metrics)\n",
    "\n",
    "joblib.dump(lgb_fitted, MODEL_DIR / \"lightgbm_ml_baseline.pkl\")\n",
    "prediction_frame = pd.DataFrame(\n",
    "    {\n",
    "        \"probability\": lgb_probs,\n",
    "        \"signal\": lgb_signal,\n",
    "        \"future_return\": y_test_returns.loc[X_test.index],\n",
    "    },\n",
    "    index=X_test.index,\n",
    ")\n",
    "prediction_frame[\"position\"] = np.sign(prediction_frame[\"signal\"])\n",
    "prediction_frame[\"pnl\"] = prediction_frame[\"position\"] * prediction_frame[\"future_return\"]\n",
    "prediction_path = PREDICTION_DIR / \"lightgbm_ml_baseline_predictions.csv\"\n",
    "prediction_frame.to_csv(prediction_path, index_label=\"time\")\n",
    "prediction_frame.head()\n",
    "\n",
    "try:\n",
    "    probability_bins = pd.qcut(\n",
    "        prediction_frame[\"probability\"], q=10, duplicates=\"drop\"\n",
    "    )\n",
    "except ValueError:\n",
    "    probability_bins = pd.cut(prediction_frame[\"probability\"], bins=5)\n",
    "probability_calibration = (\n",
    "    prediction_frame.assign(prob_bucket=probability_bins)\n",
    "    .groupby(\"prob_bucket\", observed=False)\n",
    "    .agg(\n",
    "        sample_size=(\"future_return\", \"size\"),\n",
    "        avg_future_return=(\"future_return\", \"mean\"),\n",
    "        avg_signal=(\"signal\", \"mean\"),\n",
    "        avg_position=(\"position\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "lgb_metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"cv_accuracy\": [lgb_cv_metrics.get('cv_accuracy')],\n",
    "        \"cv_auc\": [lgb_cv_metrics.get('cv_auc')],\n",
    "        \"test_accuracy\": [lgb_test_metrics.get('accuracy')],\n",
    "        \"test_roc_auc\": [lgb_test_metrics.get('roc_auc')],\n",
    "        \"test_signal_sharpe\": [lgb_test_metrics.get('signal_sharpe')],\n",
    "    },\n",
    "    index=pd.Index([\"lightgbm\"], name=\"model\"),\n",
    ")\n",
    "lgb_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2aaa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil mengekspor 13 sheet ke C:\\Users\\jefri\\backtest\\outputs\\result-test\\ml_baseline.xlsx (engine: openpyxl)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/jefri/backtest/outputs/result-test/ml_baseline.xlsx')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:  # pragma: no cover - notebook convenience\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "\n",
    "\n",
    "def export_tables_to_excel(tables, path: Path) -> Path:\n",
    "    def strip_timezone_from_value(value):\n",
    "        if value is pd.NaT:\n",
    "            return value\n",
    "        if isinstance(value, pd.Timestamp):\n",
    "            if value.tz is not None:\n",
    "                return value.tz_convert(\"UTC\").tz_localize(None)\n",
    "            return value\n",
    "        if isinstance(value, datetime):\n",
    "            if value.tzinfo is not None:\n",
    "                return value.astimezone(timezone.utc).replace(tzinfo=None)\n",
    "            return value\n",
    "        return value\n",
    "\n",
    "    def strip_timezone_from_axis(axis):\n",
    "        if isinstance(axis, pd.MultiIndex):\n",
    "            new_levels = [strip_timezone_from_axis(level) for level in axis.levels]\n",
    "            return axis.set_levels(new_levels)\n",
    "        if isinstance(axis, pd.DatetimeIndex) and axis.tz is not None:\n",
    "            return axis.tz_convert(\"UTC\").tz_localize(None)\n",
    "        if getattr(axis, \"dtype\", None) == object:\n",
    "            return pd.Index([strip_timezone_from_value(val) for val in axis], name=axis.name)\n",
    "        return axis\n",
    "\n",
    "    def make_excel_safe(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "        frame = frame.copy()\n",
    "        frame.index = strip_timezone_from_axis(frame.index)\n",
    "        frame.columns = strip_timezone_from_axis(frame.columns)\n",
    "        for column in frame.columns:\n",
    "            series = frame[column]\n",
    "            if isinstance(series.dtype, pd.DatetimeTZDtype):\n",
    "                frame[column] = series.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "            elif series.dtype == object:\n",
    "                frame[column] = series.map(strip_timezone_from_value)\n",
    "        return frame\n",
    "\n",
    "    serialisable = []\n",
    "    for sheet_name, table in tables.items():\n",
    "        if table is None:\n",
    "            continue\n",
    "        if isinstance(table, pd.Series):\n",
    "            frame = table.to_frame()\n",
    "        elif isinstance(table, pd.DataFrame):\n",
    "            frame = table.copy()\n",
    "        elif isinstance(table, dict):\n",
    "            frame = pd.DataFrame([table])\n",
    "        else:\n",
    "            frame = pd.DataFrame(table)\n",
    "        frame = make_excel_safe(frame)\n",
    "        serialisable.append((sheet_name, frame))\n",
    "\n",
    "    if not serialisable:\n",
    "        raise ValueError(\"Tidak ada tabel yang bisa diekspor.\")\n",
    "\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def pick_engine() -> str:\n",
    "        for candidate in (\"openpyxl\", \"xlsxwriter\"):\n",
    "            if importlib.util.find_spec(candidate):\n",
    "                return candidate\n",
    "        raise ModuleNotFoundError(\n",
    "            \"Untuk ekspor Excel diperlukan paket 'openpyxl' atau 'xlsxwriter'.\"\n",
    "        )\n",
    "\n",
    "    def normalise_sheet_name(name: str, existing) -> str:\n",
    "        safe = (name or \"Sheet\").strip() or \"Sheet\"\n",
    "        safe = safe[:31]\n",
    "        counter = 1\n",
    "        candidate = safe\n",
    "        while candidate in existing:\n",
    "            suffix = f\"_{counter}\"\n",
    "            trimmed = safe[: 31 - len(suffix)] or \"Sheet\"\n",
    "            candidate = f\"{trimmed}{suffix}\"\n",
    "            counter += 1\n",
    "        existing.add(candidate)\n",
    "        return candidate\n",
    "\n",
    "    engine = pick_engine()\n",
    "    used_names = set()\n",
    "    with pd.ExcelWriter(path, engine=engine) as writer:\n",
    "        for sheet_name, frame in serialisable:\n",
    "            name = normalise_sheet_name(str(sheet_name), used_names)\n",
    "            frame.to_excel(writer, sheet_name=name, index=True)\n",
    "\n",
    "    print(\n",
    "        f\"Berhasil mengekspor {len(serialisable)} sheet ke {path} (engine: {engine})\"\n",
    "    )\n",
    "    return path\n",
    "export_dir = PROJECT_ROOT / \"outputs\" / \"result-test\"\n",
    "export_path = export_dir / \"ml_baseline.xlsx\"\n",
    "\n",
    "export_tables_to_excel(\n",
    "    {\n",
    "        \"dataset\": dataset,\n",
    "        \"dataset_metadata\": dataset_metadata,\n",
    "        \"train_split\": train_split,\n",
    "        \"test_split\": test_split,\n",
    "        \"cv_split_summary\": cv_split_summary,\n",
    "        \"train_fold_performance\": train_fold_performance,\n",
    "        \"test_fold_performance\": test_fold_performance,\n",
    "        \"final_train_window\": train,\n",
    "        \"final_test_window\": test,\n",
    "        \"linear_model_metrics\": linear_metrics,\n",
    "        \"lightgbm_metrics\": lgb_metrics,\n",
    "        \"probability_calibration\": probability_calibration,\n",
    "        \"predictions\": prediction_frame,\n",
    "    },\n",
    "    export_path,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
